{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0defad-4990-4226-a74e-55dcf141ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.3.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.3.3)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (3.10.8)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.25.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.14.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting networkx>=3.0 (from scikit-image)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image)\n",
      "  Downloading imageio-2.37.2-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Downloading tifffile-2025.10.16-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m163.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m153.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m147.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_image-0.25.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading imbalanced_learn-0.14.0-py3-none-any.whl (239 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m216.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading imageio-2.37.2-py3-none-any.whl (317 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m148.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2025.10.16-py3-none-any.whl (231 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, networkx, lazy-loader, joblib, tifffile, scipy, opencv-python, imageio, scikit-learn, scikit-image, seaborn, imbalanced-learn\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.3.5\n",
      "\u001b[2K    Uninstalling numpy-2.3.5:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.5━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/13\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [imbalanced-learn][seaborn]mage]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed imageio-2.37.2 imbalanced-learn-0.14.0 joblib-1.5.2 lazy-loader-0.4 networkx-3.6.1 numpy-2.2.6 opencv-python-4.12.0.88 scikit-image-0.25.2 scikit-learn-1.8.0 scipy-1.16.3 seaborn-0.13.2 threadpoolctl-3.6.0 tifffile-2025.10.16\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.9.1%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.24.1%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.9.1%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.9.1%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (184.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchvision-0.24.1%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m160.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchaudio-2.9.1%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (495 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, torch, torchvision, torchaudio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [torchaudio]5\u001b[0m [torchaudio]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed mpmath-1.3.0 sympy-1.14.0 torch-2.9.1+cpu torchaudio-2.9.1+cpu torchvision-0.24.1+cpu\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (11.3.0)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scikit-learn seaborn matplotlib opencv-python scikit-image imbalanced-learn tqdm\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install pillow\n",
    "!pip install einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4db134d-015c-48c0-ba3a-e3c6538ebaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1924, 16)\n",
      "Index(['image', 'roi_path', 'jet_area', 'jet_width', 'jet_height',\n",
      "       'jet_length', 'red_ratio', 'orange_yellow_ratio', 'blue_ratio', 'PS',\n",
      "       'ED', 'Side', 'prox_ica_right_stenosis', 'prox_ica_left_stenosis',\n",
      "       'detected_side_value', 'Patient_Number'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/MASTER_DATASET.csv\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19de1c7b-d4e1-471c-90ea-39981ece3e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected_side_value\n",
      "0.0    1454\n",
      "1.0     201\n",
      "2.0     192\n",
      "NaN      68\n",
      "3.0       9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"detected_side_value\"].value_counts(dropna=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9fca0d5-64fc-469d-a711-692f945634f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "detected_side_value\n",
      "0.0    1454\n",
      "1.0     201\n",
      "2.0     192\n",
      "NaN      68\n",
      "3.0       9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your frozen dataset\n",
    "df = pd.read_csv(\"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/MASTER_DATASET.csv\")\n",
    "\n",
    "print(\"Before cleaning:\")\n",
    "print(df[\"detected_side_value\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "030de746-02bc-4e94-9506-c38547c6fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numeric (safe)\n",
    "df[\"detected_side_value\"] = pd.to_numeric(df[\"detected_side_value\"], errors=\"coerce\")\n",
    "\n",
    "# Merge class 3 into class 2\n",
    "df.loc[df[\"detected_side_value\"] == 3, \"detected_side_value\"] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2c1289-3c72-40ee-8df4-05aa95f0fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows without stenosis label\n",
    "df = df.dropna(subset=[\"detected_side_value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a167654-842b-40ad-bb49-6d517da845cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"detected_side_value\"] = df[\"detected_side_value\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e33f77-f030-45c2-9b67-1a8abfbe9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"final_stenosis_label\"] = df[\"detected_side_value\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b74ea1b-aee8-4593-8845-6a9c04bba40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After cleaning:\n",
      "final_stenosis_label\n",
      "0    1454\n",
      "1     201\n",
      "2     201\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAfter cleaning:\")\n",
    "print(df[\"final_stenosis_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77d7c4db-2d5a-4d45-b2c1-da6bd2ea28ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ STEP 2 complete. Dataset saved.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\n",
    "    \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/FINAL_DATASET_CLEAN.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"✅ STEP 2 complete. Dataset saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a19bda20-f283-4505-b62e-be24458d5dcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Record2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Ensure correct types\u001b[39;00m\n\u001b[32m     11\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mfinal_stenosis_label\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mfinal_stenosis_label\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mPatient_Number\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPatient_Number\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTotal samples:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df))\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUnique patients:\u001b[39m\u001b[33m\"\u001b[39m, df[\u001b[33m\"\u001b[39m\u001b[33mPatient_Number\u001b[39m\u001b[33m\"\u001b[39m].nunique())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/core/generic.py:6665\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6659\u001b[39m     results = [\n\u001b[32m   6660\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6661\u001b[39m     ]\n\u001b[32m   6663\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6664\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6665\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6666\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6667\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/core/internals/managers.py:449\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    447\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/core/internals/managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/core/internals/blocks.py:784\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    781\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    782\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    788\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:133\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arr.astype(dtype, copy=copy)\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: 'Record2'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# Load frozen dataset\n",
    "df = pd.read_csv(\"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/FINAL_DATASET_CLEAN.csv\")\n",
    "\n",
    "# Drop rows without label (you already decided this)\n",
    "df = df.dropna(subset=[\"final_stenosis_label\"])\n",
    "\n",
    "# Ensure correct types\n",
    "df[\"final_stenosis_label\"] = df[\"final_stenosis_label\"].astype(int)\n",
    "df[\"Patient_Number\"] = df[\"Patient_Number\"].astype(int)\n",
    "\n",
    "print(\"Total samples:\", len(df))\n",
    "print(\"Unique patients:\", df[\"Patient_Number\"].nunique())\n",
    "print(\"Label distribution:\\n\", df[\"final_stenosis_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92bb2f49-9dd2-4609-8e31-80b4c4431eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_patient_id(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "\n",
    "    x = str(x).strip().lower()\n",
    "\n",
    "    # Case 1: \"record 5\"\n",
    "    match = re.search(r\"record\\s*(\\d+)\", x)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "\n",
    "    # Case 2: plain number \"8\"\n",
    "    if x.isdigit():\n",
    "        return int(x)\n",
    "\n",
    "    # Anything else is invalid\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef452fae-8242-49da-bceb-ff9eb50df28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"patient_id\"] = df[\"Patient_Number\"].apply(normalize_patient_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4a7e750-1e3d-4987-86ff-5a002a16b4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 0    29\n",
      "1    29\n",
      "2    29\n",
      "3    29\n",
      "4    29\n",
      "Name: Patient_Number, dtype: object\n",
      "After : 0    29\n",
      "1    29\n",
      "2    29\n",
      "3    29\n",
      "4    29\n",
      "Name: patient_id, dtype: int64\n",
      "\n",
      "Missing patient_id: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", df[\"Patient_Number\"].head())\n",
    "print(\"After :\", df[\"patient_id\"].head())\n",
    "\n",
    "print(\"\\nMissing patient_id:\", df[\"patient_id\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c9f6980-e3f7-4ba0-afe5-346ddce0479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1856\n",
      "Unique patients: 50\n",
      "Label distribution:\n",
      " final_stenosis_label\n",
      "0    1454\n",
      "1     201\n",
      "2     201\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/FINAL_DATASET_CLEAN.csv\"\n",
    ")\n",
    "\n",
    "# Drop only rows without label (as you decided)\n",
    "df = df.dropna(subset=[\"final_stenosis_label\"])\n",
    "df[\"final_stenosis_label\"] = df[\"final_stenosis_label\"].astype(int)\n",
    "\n",
    "# --- CREATE patient_id safely ---\n",
    "df[\"Patient_Number\"] = df[\"Patient_Number\"].astype(str)\n",
    "\n",
    "df[\"patient_id\"] = (\n",
    "    df[\"Patient_Number\"]\n",
    "    .str.extract(r\"(\\d+)\", expand=False)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Sanity checks\n",
    "print(\"Total samples:\", len(df))\n",
    "print(\"Unique patients:\", df[\"patient_id\"].nunique())\n",
    "print(\"Label distribution:\\n\", df[\"final_stenosis_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "660ff1f2-ebf4-4cf4-89ee-d6dc1ecb94af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Val samples: 1462\n",
      "Test samples: 394\n",
      "Test patients: 11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "X = df.index\n",
    "y = df[\"final_stenosis_label\"]\n",
    "groups = df[\"patient_id\"]\n",
    "\n",
    "sgkf = StratifiedGroupKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "trainval_idx, test_idx = next(sgkf.split(X, y, groups))\n",
    "\n",
    "trainval_df = df.iloc[trainval_idx].reset_index(drop=True)\n",
    "test_df     = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"Train+Val samples:\", len(trainval_df))\n",
    "print(\"Test samples:\", len(test_df))\n",
    "print(\"Test patients:\", test_df[\"patient_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "905a03dd-0b02-47c6-a384-9e90397057ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1205\n",
      "Val samples: 257\n"
     ]
    }
   ],
   "source": [
    "X_tv = trainval_df.index\n",
    "y_tv = trainval_df[\"final_stenosis_label\"]\n",
    "groups_tv = trainval_df[\"patient_id\"]\n",
    "\n",
    "sgkf2 = StratifiedGroupKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_idx, val_idx = next(sgkf2.split(X_tv, y_tv, groups_tv))\n",
    "\n",
    "train_df = trainval_df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = trainval_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"Train samples:\", len(train_df))\n",
    "print(\"Val samples:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2208ee4-e74a-4104-89db-48ff712b7a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No patient leakage — STEP 3 COMPLETE\n"
     ]
    }
   ],
   "source": [
    "assert set(train_df.patient_id).isdisjoint(val_df.patient_id)\n",
    "assert set(train_df.patient_id).isdisjoint(test_df.patient_id)\n",
    "assert set(val_df.patient_id).isdisjoint(test_df.patient_id)\n",
    "\n",
    "print(\"✅ No patient leakage — STEP 3 COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23153823-8091-4837-8b08-9ba609574004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN samples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>roi_path</th>\n",
       "      <th>jet_area</th>\n",
       "      <th>jet_width</th>\n",
       "      <th>jet_height</th>\n",
       "      <th>jet_length</th>\n",
       "      <th>red_ratio</th>\n",
       "      <th>orange_yellow_ratio</th>\n",
       "      <th>blue_ratio</th>\n",
       "      <th>PS</th>\n",
       "      <th>ED</th>\n",
       "      <th>Side</th>\n",
       "      <th>prox_ica_right_stenosis</th>\n",
       "      <th>prox_ica_left_stenosis</th>\n",
       "      <th>detected_side_value</th>\n",
       "      <th>Patient_Number</th>\n",
       "      <th>final_stenosis_label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240908...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>391</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>38.078866</td>\n",
       "      <td>39.05</td>\n",
       "      <td>32.57</td>\n",
       "      <td>5.14</td>\n",
       "      <td>57.12</td>\n",
       "      <td>20.83</td>\n",
       "      <td>Right</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240908...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>22</td>\n",
       "      <td>46.529560</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>66.52</td>\n",
       "      <td>118.41</td>\n",
       "      <td>32.13</td>\n",
       "      <td>Right</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240908...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>521</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>39.446166</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>76.62</td>\n",
       "      <td>121.98</td>\n",
       "      <td>33.32</td>\n",
       "      <td>Right</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240908...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>17550</td>\n",
       "      <td>296</td>\n",
       "      <td>153</td>\n",
       "      <td>333.204142</td>\n",
       "      <td>14.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240908...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>12030</td>\n",
       "      <td>196</td>\n",
       "      <td>137</td>\n",
       "      <td>239.133854</td>\n",
       "      <td>23.28</td>\n",
       "      <td>5.47</td>\n",
       "      <td>16.64</td>\n",
       "      <td>108.20</td>\n",
       "      <td>31.43</td>\n",
       "      <td>Right</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  \\\n",
       "0  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240908...   \n",
       "1  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240908...   \n",
       "2  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240908...   \n",
       "3  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240908...   \n",
       "4  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240908...   \n",
       "\n",
       "                                            roi_path  jet_area  jet_width  \\\n",
       "0  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...       391         15   \n",
       "1  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...       608         41   \n",
       "2  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...       521         20   \n",
       "3  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...     17550        296   \n",
       "4  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...     12030        196   \n",
       "\n",
       "   jet_height  jet_length  red_ratio  orange_yellow_ratio  blue_ratio      PS  \\\n",
       "0          35   38.078866      39.05                32.57        5.14   57.12   \n",
       "1          22   46.529560       0.00                 0.00       66.52  118.41   \n",
       "2          34   39.446166       0.29                 0.00       76.62  121.98   \n",
       "3         153  333.204142      14.12                 0.00       24.68     NaN   \n",
       "4         137  239.133854      23.28                 5.47       16.64  108.20   \n",
       "\n",
       "      ED   Side  prox_ica_right_stenosis  prox_ica_left_stenosis  \\\n",
       "0  20.83  Right                      0.0                     NaN   \n",
       "1  32.13  Right                      0.0                     NaN   \n",
       "2  33.32  Right                      0.0                     NaN   \n",
       "3    NaN  Right                      0.0                     NaN   \n",
       "4  31.43  Right                      0.0                     NaN   \n",
       "\n",
       "   detected_side_value Patient_Number  final_stenosis_label  patient_id  \n",
       "0                    0             29                     0          29  \n",
       "1                    0             29                     0          29  \n",
       "2                    0             29                     0          29  \n",
       "3                    0             29                     0          29  \n",
       "4                    0             29                     0          29  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION samples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>roi_path</th>\n",
       "      <th>jet_area</th>\n",
       "      <th>jet_width</th>\n",
       "      <th>jet_height</th>\n",
       "      <th>jet_length</th>\n",
       "      <th>red_ratio</th>\n",
       "      <th>orange_yellow_ratio</th>\n",
       "      <th>blue_ratio</th>\n",
       "      <th>PS</th>\n",
       "      <th>ED</th>\n",
       "      <th>Side</th>\n",
       "      <th>prox_ica_right_stenosis</th>\n",
       "      <th>prox_ica_left_stenosis</th>\n",
       "      <th>detected_side_value</th>\n",
       "      <th>Patient_Number</th>\n",
       "      <th>final_stenosis_label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240912...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>9746</td>\n",
       "      <td>233</td>\n",
       "      <td>59</td>\n",
       "      <td>240.353906</td>\n",
       "      <td>68.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>98.91</td>\n",
       "      <td>24.23</td>\n",
       "      <td>Right</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240912...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>12051</td>\n",
       "      <td>240</td>\n",
       "      <td>146</td>\n",
       "      <td>280.919917</td>\n",
       "      <td>14.39</td>\n",
       "      <td>0.92</td>\n",
       "      <td>18.73</td>\n",
       "      <td>81.60</td>\n",
       "      <td>38.99</td>\n",
       "      <td>Left</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240912...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>727</td>\n",
       "      <td>29</td>\n",
       "      <td>45</td>\n",
       "      <td>53.535035</td>\n",
       "      <td>37.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>48.05</td>\n",
       "      <td>22.21</td>\n",
       "      <td>Left</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240912...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>3176</td>\n",
       "      <td>118</td>\n",
       "      <td>66</td>\n",
       "      <td>135.203550</td>\n",
       "      <td>1.43</td>\n",
       "      <td>3.89</td>\n",
       "      <td>35.50</td>\n",
       "      <td>74.68</td>\n",
       "      <td>35.11</td>\n",
       "      <td>Right</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240912...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>7507</td>\n",
       "      <td>167</td>\n",
       "      <td>104</td>\n",
       "      <td>196.735864</td>\n",
       "      <td>34.09</td>\n",
       "      <td>1.79</td>\n",
       "      <td>5.92</td>\n",
       "      <td>66.27</td>\n",
       "      <td>27.70</td>\n",
       "      <td>Left</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  \\\n",
       "0  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240912...   \n",
       "1  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240912...   \n",
       "2  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240912...   \n",
       "3  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240912...   \n",
       "4  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240912...   \n",
       "\n",
       "                                            roi_path  jet_area  jet_width  \\\n",
       "0  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...      9746        233   \n",
       "1  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...     12051        240   \n",
       "2  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...       727         29   \n",
       "3  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...      3176        118   \n",
       "4  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...      7507        167   \n",
       "\n",
       "   jet_height  jet_length  red_ratio  orange_yellow_ratio  blue_ratio     PS  \\\n",
       "0          59  240.353906      68.86                 0.00        0.00  98.91   \n",
       "1         146  280.919917      14.39                 0.92       18.73  81.60   \n",
       "2          45   53.535035      37.93                 0.00       24.75  48.05   \n",
       "3          66  135.203550       1.43                 3.89       35.50  74.68   \n",
       "4         104  196.735864      34.09                 1.79        5.92  66.27   \n",
       "\n",
       "      ED   Side  prox_ica_right_stenosis  prox_ica_left_stenosis  \\\n",
       "0  24.23  Right                      0.0                     0.0   \n",
       "1  38.99   Left                      0.0                     0.0   \n",
       "2  22.21   Left                      0.0                     0.0   \n",
       "3  35.11  Right                      0.0                     0.0   \n",
       "4  27.70   Left                      0.0                     0.0   \n",
       "\n",
       "   detected_side_value Patient_Number  final_stenosis_label  patient_id  \n",
       "0                    0             11                     0          11  \n",
       "1                    0             11                     0          11  \n",
       "2                    0             11                     0          11  \n",
       "3                    0             11                     0          11  \n",
       "4                    0             11                     0          11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST samples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>roi_path</th>\n",
       "      <th>jet_area</th>\n",
       "      <th>jet_width</th>\n",
       "      <th>jet_height</th>\n",
       "      <th>jet_length</th>\n",
       "      <th>red_ratio</th>\n",
       "      <th>orange_yellow_ratio</th>\n",
       "      <th>blue_ratio</th>\n",
       "      <th>PS</th>\n",
       "      <th>ED</th>\n",
       "      <th>Side</th>\n",
       "      <th>prox_ica_right_stenosis</th>\n",
       "      <th>prox_ica_left_stenosis</th>\n",
       "      <th>detected_side_value</th>\n",
       "      <th>Patient_Number</th>\n",
       "      <th>final_stenosis_label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240918...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>677</td>\n",
       "      <td>35</td>\n",
       "      <td>34</td>\n",
       "      <td>48.795492</td>\n",
       "      <td>72.27</td>\n",
       "      <td>35.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>67.20</td>\n",
       "      <td>23.74</td>\n",
       "      <td>Left</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240918...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>2196</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>82.073138</td>\n",
       "      <td>31.13</td>\n",
       "      <td>16.49</td>\n",
       "      <td>23.18</td>\n",
       "      <td>48.96</td>\n",
       "      <td>9.45</td>\n",
       "      <td>Left</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240918...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>880</td>\n",
       "      <td>47</td>\n",
       "      <td>28</td>\n",
       "      <td>54.708317</td>\n",
       "      <td>63.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>72.93</td>\n",
       "      <td>14.79</td>\n",
       "      <td>Right</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240918...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>660</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>53.814496</td>\n",
       "      <td>21.32</td>\n",
       "      <td>27.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>66.64</td>\n",
       "      <td>21.64</td>\n",
       "      <td>Left</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DICOM\\1.3.12.2.1107.5.5.17.820288.300000240918...</td>\n",
       "      <td>/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...</td>\n",
       "      <td>3242</td>\n",
       "      <td>89</td>\n",
       "      <td>50</td>\n",
       "      <td>102.083299</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>73.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Left</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  \\\n",
       "0  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240918...   \n",
       "1  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240918...   \n",
       "2  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240918...   \n",
       "3  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240918...   \n",
       "4  DICOM\\1.3.12.2.1107.5.5.17.820288.300000240918...   \n",
       "\n",
       "                                            roi_path  jet_area  jet_width  \\\n",
       "0  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...       677         35   \n",
       "1  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...      2196         60   \n",
       "2  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...       880         47   \n",
       "3  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...       660         40   \n",
       "4  /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/...      3242         89   \n",
       "\n",
       "   jet_height  jet_length  red_ratio  orange_yellow_ratio  blue_ratio     PS  \\\n",
       "0          34   48.795492      72.27                35.55        0.00  67.20   \n",
       "1          56   82.073138      31.13                16.49       23.18  48.96   \n",
       "2          28   54.708317      63.75                 0.00        0.00  72.93   \n",
       "3          36   53.814496      21.32                27.64        0.00  66.64   \n",
       "4          50  102.083299       0.00                 0.00       73.19    NaN   \n",
       "\n",
       "      ED   Side  prox_ica_right_stenosis  prox_ica_left_stenosis  \\\n",
       "0  23.74   Left                      0.0                     1.0   \n",
       "1   9.45   Left                      0.0                     1.0   \n",
       "2  14.79  Right                      0.0                     1.0   \n",
       "3  21.64   Left                      0.0                     1.0   \n",
       "4    NaN   Left                      0.0                     1.0   \n",
       "\n",
       "   detected_side_value Patient_Number  final_stenosis_label  patient_id  \n",
       "0                    1              8                     1           8  \n",
       "1                    1              8                     1           8  \n",
       "2                    0              8                     0           8  \n",
       "3                    1              8                     1           8  \n",
       "4                    1              8                     1           8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"TRAIN samples:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nVALIDATION samples:\")\n",
    "display(val_df.head())\n",
    "\n",
    "print(\"\\nTEST samples:\")\n",
    "display(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fdc438f-205a-4c5f-b2b4-adb2eb2c9224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train patients: [2, 4, 7, 9, 10, 13, 15, 16, 17, 21, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "Val patients: [6, 11, 12, 14, 19, 26, 35]\n",
      "Test patients: [1, 3, 5, 8, 18, 20, 22, 25, 36, 40, 42]\n"
     ]
    }
   ],
   "source": [
    "train_patients = set(train_df[\"patient_id\"])\n",
    "val_patients   = set(val_df[\"patient_id\"])\n",
    "test_patients  = set(test_df[\"patient_id\"])\n",
    "\n",
    "print(\"Train patients:\", sorted(train_patients))\n",
    "print(\"Val patients:\", sorted(val_patients))\n",
    "print(\"Test patients:\", sorted(test_patients))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8cdf3b3-d2e6-4224-b2c7-51ce8325292c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN SPLIT\n",
      "------------------------------\n",
      "Samples  : 1205 (64.92%)\n",
      "Patients : 32 (64.00%)\n",
      "\n",
      "VALIDATION SPLIT\n",
      "------------------------------\n",
      "Samples  : 257 (13.85%)\n",
      "Patients : 7 (14.00%)\n",
      "\n",
      "TEST SPLIT\n",
      "------------------------------\n",
      "Samples  : 394 (21.23%)\n",
      "Patients : 11 (22.00%)\n"
     ]
    }
   ],
   "source": [
    "# Total counts\n",
    "total_samples = len(df)\n",
    "total_patients = df[\"patient_id\"].nunique()\n",
    "\n",
    "def split_stats(name, split_df):\n",
    "    n_samples = len(split_df)\n",
    "    n_patients = split_df[\"patient_id\"].nunique()\n",
    "\n",
    "    print(f\"\\n{name} SPLIT\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Samples  : {n_samples} ({n_samples/total_samples*100:.2f}%)\")\n",
    "    print(f\"Patients : {n_patients} ({n_patients/total_patients*100:.2f}%)\")\n",
    "\n",
    "# Print stats\n",
    "split_stats(\"TRAIN\", train_df)\n",
    "split_stats(\"VALIDATION\", val_df)\n",
    "split_stats(\"TEST\", test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54e31f7b-b68e-4321-a463-b1ab9e9b20f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ∩ Val: set()\n",
      "Train ∩ Test: set()\n",
      "Val ∩ Test: set()\n"
     ]
    }
   ],
   "source": [
    "print(\"Train ∩ Val:\", train_patients & val_patients)\n",
    "print(\"Train ∩ Test:\", train_patients & test_patients)\n",
    "print(\"Val ∩ Test:\", val_patients & test_patients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ae17c8d-7117-4cc0-b33f-f7a5788ad778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per patient in TRAIN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "patient_id\n",
       "44    64\n",
       "2     58\n",
       "34    56\n",
       "38    52\n",
       "27    50\n",
       "46    50\n",
       "16    50\n",
       "24    49\n",
       "10    47\n",
       "33    47\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples per patient in TEST:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "patient_id\n",
       "42    79\n",
       "18    42\n",
       "5     38\n",
       "40    35\n",
       "20    34\n",
       "8     32\n",
       "22    31\n",
       "1     28\n",
       "36    27\n",
       "3     25\n",
       "25    23\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Samples per patient in TRAIN:\")\n",
    "display(train_df[\"patient_id\"].value_counts().head(10))\n",
    "\n",
    "print(\"\\nSamples per patient in TEST:\")\n",
    "display(test_df[\"patient_id\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f2ed65c-7f7c-43a8-b5d5-abc2c59da79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Split</th>\n",
       "      <th>Samples</th>\n",
       "      <th>Sample %</th>\n",
       "      <th>Patients</th>\n",
       "      <th>Patient %</th>\n",
       "      <th>Patient IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>1205</td>\n",
       "      <td>64.924569</td>\n",
       "      <td>32</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2, 4, 7, 9, 10, 13, 15, 16, 17, 21, 23, 24, 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Validation</td>\n",
       "      <td>257</td>\n",
       "      <td>13.846983</td>\n",
       "      <td>7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6, 11, 12, 14, 19, 26, 35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test</td>\n",
       "      <td>394</td>\n",
       "      <td>21.228448</td>\n",
       "      <td>11</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1, 3, 5, 8, 18, 20, 22, 25, 36, 40, 42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Split  Samples   Sample %  Patients  Patient %  \\\n",
       "0       Train     1205  64.924569        32       64.0   \n",
       "1  Validation      257  13.846983         7       14.0   \n",
       "2        Test      394  21.228448        11       22.0   \n",
       "\n",
       "                                         Patient IDs  \n",
       "0  2, 4, 7, 9, 10, 13, 15, 16, 17, 21, 23, 24, 27...  \n",
       "1                          6, 11, 12, 14, 19, 26, 35  \n",
       "2             1, 3, 5, 8, 18, 20, 22, 25, 36, 40, 42  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary = pd.DataFrame({\n",
    "    \"Split\": [\"Train\", \"Validation\", \"Test\"],\n",
    "\n",
    "    \"Samples\": [\n",
    "        len(train_df),\n",
    "        len(val_df),\n",
    "        len(test_df)\n",
    "    ],\n",
    "\n",
    "    \"Sample %\": [\n",
    "        len(train_df) / total_samples * 100,\n",
    "        len(val_df) / total_samples * 100,\n",
    "        len(test_df) / total_samples * 100\n",
    "    ],\n",
    "\n",
    "    \"Patients\": [\n",
    "        train_df[\"patient_id\"].nunique(),\n",
    "        val_df[\"patient_id\"].nunique(),\n",
    "        test_df[\"patient_id\"].nunique()\n",
    "    ],\n",
    "\n",
    "    \"Patient %\": [\n",
    "        train_df[\"patient_id\"].nunique() / total_patients * 100,\n",
    "        val_df[\"patient_id\"].nunique() / total_patients * 100,\n",
    "        test_df[\"patient_id\"].nunique() / total_patients * 100\n",
    "    ],\n",
    "\n",
    "    # 🔹 NEW COLUMN\n",
    "    \"Patient IDs\": [\n",
    "        \", \".join(map(str, sorted(train_df[\"patient_id\"].unique()))),\n",
    "        \", \".join(map(str, sorted(val_df[\"patient_id\"].unique()))),\n",
    "        \", \".join(map(str, sorted(test_df[\"patient_id\"].unique())))\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5adf044d-9846-46ea-b228-d66e7ba47c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Splits saved\n"
     ]
    }
   ],
   "source": [
    "train_df.to_csv(\"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Files/train.csv\", index=False)\n",
    "val_df.to_csv(\"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Files/validation.csv\", index=False)\n",
    "test_df.to_csv(\"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Files/test.csv\", index=False)\n",
    "\n",
    "print(\"✅ Splits saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e98a8e2-42bc-4cbf-8238-45c34669da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63b65393-7936-4f6e-9a03-23565c038fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 1856\n",
      "Labels:\n",
      " final_stenosis_label\n",
      "0    1454\n",
      "1     201\n",
      "2     201\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/FINAL_DATASET_CLEAN.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(\"Samples:\", len(df))\n",
    "print(\"Labels:\\n\", df[\"final_stenosis_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff5fbc86-12d7-4c8c-a695-d57d7b7ca596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"side_enc\"] = df[\"Side\"].map({\n",
    "    \"Left\": 0,\n",
    "    \"Right\": 1\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac290bfc-0fe5-46bd-80bf-8e695fb0eea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "side_enc\n",
      "0    967\n",
      "1    889\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"side_enc\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "958b6fee-c570-4966-9dc7-b05c7fa3afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB_FEATURES = [\n",
    "    \"jet_area\",\n",
    "    \"jet_width\",\n",
    "    \"jet_height\",\n",
    "    \"jet_length\",\n",
    "    \"red_ratio\",\n",
    "    \"orange_yellow_ratio\",\n",
    "    \"blue_ratio\",\n",
    "    \"PS\",\n",
    "    \"ED\",\n",
    "    \"side_enc\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c228d09e-5d5c-4c29-bad6-7989959198f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet_area                 0\n",
      "jet_width                0\n",
      "jet_height               0\n",
      "jet_length               0\n",
      "red_ratio                0\n",
      "orange_yellow_ratio      0\n",
      "blue_ratio               0\n",
      "PS                     251\n",
      "ED                     310\n",
      "side_enc                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[TAB_FEATURES].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bcffcf7f-4c77-450d-a13c-097cb04f02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbbc4874-f2bb-49eb-8518-d9c0aacad6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = \"final_stenosis_label\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "612b6793-0e7f-434a-bf4b-769836ece75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PS_missing\"] = df[\"PS\"].isna().astype(int)\n",
    "df[\"ED_missing\"] = df[\"ED\"].isna().astype(int)\n",
    "\n",
    "# Now:\n",
    "# 1 = value was missing\n",
    "# 0 = value present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99a14bb4-42ea-4e77-b4d1-335b7be1944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PS\"] = df[\"PS\"].fillna(df[\"PS\"].median())\n",
    "df[\"ED\"] = df[\"ED\"].fillna(df[\"ED\"].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2daff057-af60-4da6-a54b-3836e1167f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB_FEATURES = [\n",
    "    \"jet_area\",\n",
    "    \"jet_width\",\n",
    "    \"jet_height\",\n",
    "    \"jet_length\",\n",
    "    \"red_ratio\",\n",
    "    \"orange_yellow_ratio\",\n",
    "    \"blue_ratio\",\n",
    "    \"PS\",\n",
    "    \"ED\",\n",
    "    \"PS_missing\",\n",
    "    \"ED_missing\",\n",
    "    \"side_enc\"\n",
    "]\n",
    "#side_enc = 0 ( means Left) or 1 ( means Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b010fbac-4556-40ab-ad6f-4f2d88f7183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet_area               0\n",
      "jet_width              0\n",
      "jet_height             0\n",
      "jet_length             0\n",
      "red_ratio              0\n",
      "orange_yellow_ratio    0\n",
      "blue_ratio             0\n",
      "PS                     0\n",
      "ED                     0\n",
      "PS_missing             0\n",
      "ED_missing             0\n",
      "side_enc               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[TAB_FEATURES].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cf4c48b-74f1-4ad9-b4fb-7051669ec924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Files/FINAL_DATASET_MODEL_READY.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48d985f2-26b7-4d76-b521-e85020887739",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB_FEATURES = [\n",
    "    \"jet_area\",\n",
    "    \"jet_width\",\n",
    "    \"jet_height\",\n",
    "    \"jet_length\",\n",
    "    \"red_ratio\",\n",
    "    \"orange_yellow_ratio\",\n",
    "    \"blue_ratio\",\n",
    "    \"PS\",\n",
    "    \"ED\",\n",
    "    \"side_enc\"\n",
    "]\n",
    "\n",
    "LABEL_COL = \"final_stenosis_label\"\n",
    "IMAGE_COL = \"roi_path\"        # or \"image\" (adjust if needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd0d914a-ed0a-4ba8-80d7-a18541bae4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e353625c-d76e-41c4-954e-b5068112de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Files/train.csv\"\n",
    "VAL_CSV   = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Files/validation.csv\"\n",
    "TEST_CSV  = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Files/test.csv\"\n",
    "\n",
    "ROOT_IMG  = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Extracted_roi_folder\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5d4493ac-8c80-4a53-b154-87e570106938",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_ in [train_df, val_df, test_df]:\n",
    "    df_[\"PS_missing\"] = df_[\"PS\"].isna().astype(int)\n",
    "    df_[\"ED_missing\"] = df_[\"ED\"].isna().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c3619b63-a580-4550-b3cd-c38e8a197838",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_ in [train_df, val_df, test_df]:\n",
    "    df_[\"PS\"] = df_[\"PS\"].fillna(0)\n",
    "    df_[\"ED\"] = df_[\"ED\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f2fc9596-ae99-4aec-8ef0-250fb69c4caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PS     ED  PS_missing  ED_missing\n",
      "0   57.12  20.83           0           0\n",
      "1  118.41  32.13           0           0\n",
      "2  121.98  33.32           0           0\n",
      "3    0.00   0.00           0           0\n",
      "4  108.20  31.43           0           0\n"
     ]
    }
   ],
   "source": [
    "print(train_df[[\"PS\", \"ED\", \"PS_missing\", \"ED_missing\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1765478e-ee3b-4c8a-955c-44ca4e1a567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_ in [train_df, val_df, test_df]:\n",
    "    df_[\"side_enc\"] = df_[\"Side\"].map({\"Left\": 0, \"Right\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f1760773-1ed6-4b4f-9f12-ac199d6f43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB_FEATURES = [\n",
    "    \"jet_area\",\n",
    "    \"jet_width\",\n",
    "    \"jet_height\",\n",
    "    \"jet_length\",\n",
    "    \"red_ratio\",\n",
    "    \"orange_yellow_ratio\",\n",
    "    \"blue_ratio\",\n",
    "    \"PS\",\n",
    "    \"ED\",\n",
    "    \"side_enc\",\n",
    "    \"PS_missing\",\n",
    "    \"ED_missing\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0a3b87bd-e276-42b6-892d-6068a3b0ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_df[TAB_FEATURES] = scaler.fit_transform(train_df[TAB_FEATURES])\n",
    "val_df[TAB_FEATURES]   = scaler.transform(val_df[TAB_FEATURES])\n",
    "test_df[TAB_FEATURES]  = scaler.transform(test_df[TAB_FEATURES])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d777d2d2-5249-43d6-aef4-02e0d6bca620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StenosisMultiModalDataset(Dataset):\n",
    "    def __init__(self, df, tab_features, image_col, label_col, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tab_features = tab_features\n",
    "        self.image_col = image_col\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # ---- IMAGE (direct path) ----\n",
    "        img_path = row[self.image_col]\n",
    "        if not isinstance(img_path, str) or not img_path.strip():\n",
    "            raise FileNotFoundError(f\"Missing image path at index {idx}\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # ---- TABULAR ----\n",
    "        tab = row[self.tab_features].values.astype(np.float32)\n",
    "        tab = torch.tensor(tab, dtype=torch.float32)\n",
    "\n",
    "        # ---- LABEL ----\n",
    "        label = int(row[self.label_col])\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, tab, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e63127b5-467e-48c9-bcd7-7da6079c9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c369a9ef-93af-4516-b41d-c8979bc8a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB_FEATURES = [\n",
    "    \"jet_area\", \"jet_width\", \"jet_height\", \"jet_length\",\n",
    "    \"red_ratio\", \"orange_yellow_ratio\", \"blue_ratio\",\n",
    "    \"PS\", \"ED\", \"side_enc\"\n",
    "]\n",
    "\n",
    "IMAGE_COL = \"roi_path\"\n",
    "LABEL_COL = \"final_stenosis_label\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f95b0523-2eea-49a6-ae98-e1504aa81c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_side_encoding(df):\n",
    "    df = df.copy()\n",
    "    df[\"side_enc\"] = df[\"Side\"].map({\n",
    "        \"Left\": 0,\n",
    "        \"Right\": 1\n",
    "    })\n",
    "    return df\n",
    "\n",
    "train_df = add_side_encoding(train_df)\n",
    "val_df   = add_side_encoding(val_df)\n",
    "test_df  = add_side_encoding(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2876016c-4a69-4e74-9c23-1ba4f6169f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StenosisMultiModalDataset(\n",
    "    train_df, TAB_FEATURES, IMAGE_COL, LABEL_COL, transform=image_transforms\n",
    ")\n",
    "\n",
    "val_dataset = StenosisMultiModalDataset(\n",
    "    val_df, TAB_FEATURES, IMAGE_COL, LABEL_COL, transform=image_transforms\n",
    ")\n",
    "\n",
    "test_dataset = StenosisMultiModalDataset(\n",
    "    test_df, TAB_FEATURES, IMAGE_COL, LABEL_COL, transform=image_transforms\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c4943567-224a-47c1-96a7-ffe3f5ed7563",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "712b9548-4f30-4497-a494-52b29cfc1a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([16, 3, 224, 224])\n",
      "Tabular: torch.Size([16, 10])\n",
      "Labels: tensor([0, 0, 0, 0, 0, 2, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "images, tabs, labels = next(iter(train_loader))\n",
    "\n",
    "print(\"Images:\", images.shape)      # [B, 3, 224, 224]\n",
    "print(\"Tabular:\", tabs.shape)       # [B, num_features]\n",
    "print(\"Labels:\", labels[:10])       # values: 0,1,2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c5770188-5eee-4425-8f57-6368e3abe934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2]), tensor([924, 117, 164]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "all_labels = torch.cat([labels for _,_,labels in train_loader])\n",
    "print(torch.unique(all_labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "816ebe42-cdee-4395-acd2-bcc7eea9665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e165ad4a-6680-4084-85ca-837867990b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalStenosisNet(nn.Module):\n",
    "    def __init__(self, tabular_dim, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------- IMAGE BRANCH (EfficientNet-B0) --------\n",
    "        self.cnn = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "        in_features = self.cnn.classifier[1].in_features\n",
    "        self.cnn.classifier = nn.Identity()  # remove original classifier\n",
    "\n",
    "        self.image_fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # -------- TABULAR BRANCH --------\n",
    "        self.tabular_fc = nn.Sequential(\n",
    "            nn.Linear(tabular_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # -------- FUSION CLASSIFIER --------\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 + 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, tabular):\n",
    "        img_feat = self.cnn(image)\n",
    "        img_feat = self.image_fc(img_feat)\n",
    "\n",
    "        tab_feat = self.tabular_fc(tabular)\n",
    "\n",
    "        fused = torch.cat([img_feat, tab_feat], dim=1)\n",
    "        out = self.classifier(fused)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e81dc6be-8f76-49fb-9e92-8b9f1bbbdec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.4347, 3.4330, 2.4492])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "labels = train_df[\"final_stenosis_label\"].values\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b640b038-4c31-4271-a67e-c028781fbf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet_area               0\n",
      "jet_width              0\n",
      "jet_height             0\n",
      "jet_length             0\n",
      "red_ratio              0\n",
      "orange_yellow_ratio    0\n",
      "blue_ratio             0\n",
      "PS                     0\n",
      "ED                     0\n",
      "side_enc               0\n",
      "dtype: int64\n",
      "           jet_area     jet_width    jet_height    jet_length    red_ratio  \\\n",
      "count  1.205000e+03  1.205000e+03  1.205000e+03  1.205000e+03  1205.000000   \n",
      "mean  -4.127634e-17  5.012127e-17 -5.012127e-17 -2.137525e-17     0.000000   \n",
      "std    1.000415e+00  1.000415e+00  1.000415e+00  1.000415e+00     1.000415   \n",
      "min   -6.414678e-01 -9.763130e-01 -1.212029e+00 -1.099640e+00    -1.132344   \n",
      "25%   -5.828403e-01 -7.939646e-01 -8.383893e-01 -8.339617e-01    -0.818997   \n",
      "50%   -3.868929e-01 -4.201504e-01 -2.364144e-01 -3.062186e-01    -0.263074   \n",
      "75%    1.525632e-01  6.830574e-01  6.354112e-01  6.431766e-01     0.639132   \n",
      "max    5.632192e+00  3.017117e+00  4.184987e+00  3.275238e+00     2.875732   \n",
      "\n",
      "       orange_yellow_ratio    blue_ratio            PS            ED  \\\n",
      "count         1.205000e+03  1.205000e+03  1.205000e+03  1.205000e+03   \n",
      "mean         -3.537972e-17  2.948310e-17 -7.665606e-17  8.255268e-17   \n",
      "std           1.000415e+00  1.000415e+00  1.000415e+00  1.000415e+00   \n",
      "min          -7.542261e-01 -9.364700e-01 -1.237837e+00 -6.684469e-01   \n",
      "25%          -7.542261e-01 -8.671044e-01 -4.968059e-01 -3.089600e-01   \n",
      "50%          -4.712583e-01 -3.725822e-01 -1.180567e-01 -1.213579e-01   \n",
      "75%           4.800293e-01  5.989261e-01  3.170184e-01  1.254341e-01   \n",
      "max           4.272281e+00  2.778487e+00  8.018315e+00  2.007077e+01   \n",
      "\n",
      "          side_enc  \n",
      "count  1205.000000  \n",
      "mean      0.481328  \n",
      "std       0.499859  \n",
      "min       0.000000  \n",
      "25%       0.000000  \n",
      "50%       0.000000  \n",
      "75%       1.000000  \n",
      "max       1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(train_df[TAB_FEATURES].isna().sum())\n",
    "print(train_df[TAB_FEATURES].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "63c3af43-4efd-429b-8d04-0c0a18d0d390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d90af040-e684-469d-85d5-56f46f78623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalStenosisNet(nn.Module):\n",
    "    def __init__(self, tabular_dim, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------- IMAGE BRANCH (CNN) --------\n",
    "        self.cnn = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "        cnn_out_dim = self.cnn.classifier[1].in_features\n",
    "        self.cnn.classifier = nn.Identity()  # remove classifier\n",
    "\n",
    "        self.image_fc = nn.Sequential(\n",
    "            nn.Linear(cnn_out_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "\n",
    "        # -------- TABULAR BRANCH (MLP) --------\n",
    "        self.tabular_net = nn.Sequential(\n",
    "            nn.Linear(tabular_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # -------- FUSION + CLASSIFIER --------\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 + 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, tabular):\n",
    "        img_feat = self.cnn(image)\n",
    "        img_feat = self.image_fc(img_feat)\n",
    "\n",
    "        tab_feat = self.tabular_net(tabular)\n",
    "\n",
    "        fused = torch.cat([img_feat, tab_feat], dim=1)\n",
    "        output = self.classifier(fused)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "75d38ac8-5645-48d8-9e47-24a6832413c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB_DIM = len(TAB_FEATURES)\n",
    "\n",
    "model = MultiModalStenosisNet(tabular_dim=TAB_DIM, num_classes=3)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0569faca-a621-4e12-af0b-d5519f0cc0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([0.4347, 3.4330, 2.4492], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "db9ffd9b-4f9a-47b9-9214-364f6210719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8a3ff6f6-94dd-4691-b18f-5dd2a94ff22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "01cda601-1310-4ed0-9ec1-087afe37286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.3,\n",
    "    patience=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3e0efff8-05a4-4dee-bf5e-84a58cba5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, tabs, labels in tqdm(loader, leave=False):\n",
    "        images = images.to(device)\n",
    "        tabs = tabs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, tabs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "de9455f4-656e-4fb7-95fe-e4ffee128921",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, tabs, labels in loader:\n",
    "        images = images.to(device)\n",
    "        tabs = tabs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images, tabs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a735a-81ba-40e0-abe6-1ed38b5661cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"\"\"\n",
    "Epoch {epoch+1}/{EPOCHS}\n",
    "Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f}\n",
    "Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.3f}\n",
    "\"\"\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_multimodal_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fdde248d-eae9-48bd-bac7-c0905e0557c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1964f74e-55af-4bbc-8981-35f24e8314c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 1.1057, Acc: 0.659 | Val Loss: 0.9426, Acc: 0.805\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss: 1.0457, Acc: 0.624 | Val Loss: 0.9332, Acc: 0.720\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Loss: 0.9582, Acc: 0.679 | Val Loss: 0.9236, Acc: 0.654\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Loss: 0.8526, Acc: 0.759 | Val Loss: 0.8857, Acc: 0.665\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Loss: 0.7498, Acc: 0.725 | Val Loss: 0.9011, Acc: 0.630\n",
      "⚠ No improvement for 1/5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Loss: 0.5946, Acc: 0.797 | Val Loss: 0.8333, Acc: 0.724\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Train Loss: 0.4058, Acc: 0.844 | Val Loss: 0.8119, Acc: 0.732\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Train Loss: 0.3775, Acc: 0.866 | Val Loss: 0.7479, Acc: 0.755\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Train Loss: 0.2640, Acc: 0.900 | Val Loss: 0.8959, Acc: 0.763\n",
      "⚠ No improvement for 1/5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.1816, Acc: 0.940 | Val Loss: 0.8069, Acc: 0.732\n",
      "⚠ No improvement for 2/5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train Loss: 0.1875, Acc: 0.929 | Val Loss: 0.8270, Acc: 0.770\n",
      "⚠ No improvement for 3/5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train Loss: 0.1574, Acc: 0.945 | Val Loss: 1.0287, Acc: 0.712\n",
      "⚠ No improvement for 4/5 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train Loss: 0.1184, Acc: 0.951 | Val Loss: 0.9969, Acc: 0.767\n",
      "⚠ No improvement for 5/5 epochs\n",
      "🛑 Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "\n",
    "    # Scheduler step (based on validation loss)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.3f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    # ---- CHECK IMPROVEMENT ----\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        torch.save(model.state_dict(), \"best_multimodal_model.pth\")\n",
    "        print(\"✅ Best model saved\")\n",
    "\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"⚠ No improvement for {epochs_no_improve}/{PATIENCE} epochs\")\n",
    "\n",
    "    # ---- EARLY STOPPING ----\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(\"🛑 Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0e396321-82b9-45a1-bf4b-ab8cacc0ee9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model loaded for TEST evaluation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(torch.load(\"best_multimodal_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Best model loaded for TEST evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f2bccffc-2fed-436d-8c07-f0f1cead4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, tabs, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        tabs = tabs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images, tabs)        # shape: [B, num_classes]\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "3439384f-8662-42c8-9203-2f4ff42fe2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Test Accuracy: 0.6142\n",
      "⚖ Balanced Accuracy: 0.3042\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(all_labels, all_preds)\n",
    "bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"🎯 Test Accuracy: {acc:.4f}\")\n",
    "print(f\"⚖ Balanced Accuracy: {bal_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b0035829-e3da-4ab2-b5de-d671a3337a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[236  31  38]\n",
      " [ 40   3   9]\n",
      " [ 30   4   3]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2e2fb752-becc-4e1e-ade1-3bfbf2344a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"No\", \"Moderate\", \"Severe\"],\n",
    "            yticklabels=[\"No\", \"Moderate\", \"Severe\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e94caa84-0da0-4f92-b1cf-df8f3a6ef325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " No stenosis       0.77      0.77      0.77       305\n",
      "    Moderate       0.08      0.06      0.07        52\n",
      "      Severe       0.06      0.08      0.07        37\n",
      "\n",
      "    accuracy                           0.61       394\n",
      "   macro avg       0.30      0.30      0.30       394\n",
      "weighted avg       0.61      0.61      0.61       394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=[\"No stenosis\", \"Moderate\", \"Severe\"]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "300a89a7-2e6e-481a-b532-b93e02a62b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3af2c1f1-77fb-44c1-a28b-bc64514546de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(weight=alpha)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = self.ce(logits, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9c74ccc7-a8f3-4dd4-9248-be786e8b4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalStenosisNet(nn.Module):\n",
    "    def __init__(self, tab_dim, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---- CNN branch ----\n",
    "        self.cnn = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "        in_feats = self.cnn.classifier[1].in_features\n",
    "        self.cnn.classifier = nn.Identity()\n",
    "\n",
    "        self.img_head = nn.Sequential(\n",
    "            nn.Linear(in_feats, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6)\n",
    "        )\n",
    "\n",
    "        # ---- Tabular branch ----\n",
    "        self.tabular = nn.Sequential(\n",
    "            nn.Linear(tab_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # ---- Fusion ----\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 + 32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, tab):\n",
    "        img_feat = self.cnn(image)\n",
    "        img_feat = self.img_head(img_feat)\n",
    "        tab_feat = self.tabular(tab)\n",
    "        fused = torch.cat([img_feat, tab_feat], dim=1)\n",
    "        return self.classifier(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a5309d74-4fa8-4eb2-909a-3a2a2304fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_backbone(model):\n",
    "    for param in model.cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_backbone(model):\n",
    "    for param in model.cnn.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2acfd5af-ae0c-474b-a055-af44189a6984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: tensor([0.0688, 0.5435, 0.3877])\n"
     ]
    }
   ],
   "source": [
    "# example: your counts (adjust if needed)\n",
    "class_counts = torch.tensor([924, 117, 164], dtype=torch.float)\n",
    "weights = 1.0 / class_counts\n",
    "weights = weights / weights.sum()\n",
    "weights = torch.clamp(weights, max=2.0)  # IMPORTANT\n",
    "weights = weights.to(device)\n",
    "\n",
    "print(\"Using class weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bd025123-c8e3-4643-9adf-4d2108f48124",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultiModalStenosisNet(tab_dim=len(TAB_FEATURES)).to(device)\n",
    "\n",
    "criterion = FocalLoss(alpha=weights, gamma=2.0)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=3e-5,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4976516b-a55d-4d01-9259-32cd3eef159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for images, tabs, labels in tqdm(loader, leave=False):\n",
    "        images, tabs, labels = images.to(device), tabs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, tabs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for images, tabs, labels in loader:\n",
    "        images, tabs, labels = images.to(device), tabs.to(device), labels.to(device)\n",
    "        outputs = model(images, tabs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "78009217-6ac4-447c-afbb-f0530beb243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔒 CNN backbone frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n",
      "Train Loss: 0.5302 | Acc: 0.309\n",
      "Val   Loss: 0.4776 | Acc: 0.327\n",
      "\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/30\n",
      "Train Loss: 0.5161 | Acc: 0.340\n",
      "Val   Loss: 0.4755 | Acc: 0.335\n",
      "\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/30\n",
      "Train Loss: 0.5250 | Acc: 0.337\n",
      "Val   Loss: 0.4623 | Acc: 0.451\n",
      "\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/30\n",
      "Train Loss: 0.5222 | Acc: 0.368\n",
      "Val   Loss: 0.4620 | Acc: 0.424\n",
      "\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/30\n",
      "Train Loss: 0.5101 | Acc: 0.366\n",
      "Val   Loss: 0.4559 | Acc: 0.490\n",
      "\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/30\n",
      "Train Loss: 0.4910 | Acc: 0.389\n",
      "Val   Loss: 0.4564 | Acc: 0.463\n",
      "\n",
      "⚠ No improvement 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/30\n",
      "Train Loss: 0.5021 | Acc: 0.405\n",
      "Val   Loss: 0.4354 | Acc: 0.560\n",
      "\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/30\n",
      "Train Loss: 0.5182 | Acc: 0.372\n",
      "Val   Loss: 0.4442 | Acc: 0.533\n",
      "\n",
      "⚠ No improvement 1/5\n",
      "🔓 CNN backbone unfrozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/30\n",
      "Train Loss: 0.4830 | Acc: 0.410\n",
      "Val   Loss: 0.4412 | Acc: 0.553\n",
      "\n",
      "⚠ No improvement 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/30\n",
      "Train Loss: 0.4768 | Acc: 0.422\n",
      "Val   Loss: 0.4358 | Acc: 0.568\n",
      "\n",
      "⚠ No improvement 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/30\n",
      "Train Loss: 0.4679 | Acc: 0.427\n",
      "Val   Loss: 0.4258 | Acc: 0.626\n",
      "\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/30\n",
      "Train Loss: 0.4440 | Acc: 0.466\n",
      "Val   Loss: 0.4228 | Acc: 0.615\n",
      "\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/30\n",
      "Train Loss: 0.4447 | Acc: 0.472\n",
      "Val   Loss: 0.4238 | Acc: 0.619\n",
      "\n",
      "⚠ No improvement 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/30\n",
      "Train Loss: 0.4366 | Acc: 0.490\n",
      "Val   Loss: 0.4135 | Acc: 0.603\n",
      "\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/30\n",
      "Train Loss: 0.4169 | Acc: 0.510\n",
      "Val   Loss: 0.4136 | Acc: 0.615\n",
      "\n",
      "⚠ No improvement 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/30\n",
      "Train Loss: 0.4482 | Acc: 0.471\n",
      "Val   Loss: 0.4028 | Acc: 0.603\n",
      "\n",
      "✅ Best model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/30\n",
      "Train Loss: 0.3934 | Acc: 0.528\n",
      "Val   Loss: 0.4138 | Acc: 0.615\n",
      "\n",
      "⚠ No improvement 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/30\n",
      "Train Loss: 0.4071 | Acc: 0.512\n",
      "Val   Loss: 0.4134 | Acc: 0.588\n",
      "\n",
      "⚠ No improvement 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/30\n",
      "Train Loss: 0.3789 | Acc: 0.552\n",
      "Val   Loss: 0.4179 | Acc: 0.595\n",
      "\n",
      "⚠ No improvement 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/30\n",
      "Train Loss: 0.3685 | Acc: 0.552\n",
      "Val   Loss: 0.4164 | Acc: 0.541\n",
      "\n",
      "⚠ No improvement 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/30\n",
      "Train Loss: 0.3768 | Acc: 0.558\n",
      "Val   Loss: 0.4091 | Acc: 0.591\n",
      "\n",
      "⚠ No improvement 5/5\n",
      "🛑 Early stopping\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "FREEZE_EPOCHS = 8\n",
    "best_val_loss = float(\"inf\")\n",
    "patience, wait = 5, 0\n",
    "\n",
    "freeze_backbone(model)\n",
    "print(\"🔒 CNN backbone frozen\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    if epoch == FREEZE_EPOCHS:\n",
    "        unfreeze_backbone(model)\n",
    "        print(\"🔓 CNN backbone unfrozen\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"\"\"\n",
    "Epoch {epoch+1}/{EPOCHS}\n",
    "Train Loss: {train_loss:.4f} | Acc: {train_acc:.3f}\n",
    "Val   Loss: {val_loss:.4f} | Acc: {val_acc:.3f}\n",
    "\"\"\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), \"best_multimodal_focal_regularized.pth\")\n",
    "        print(\"✅ Best model saved\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        print(f\"⚠ No improvement {wait}/{patience}\")\n",
    "\n",
    "    if wait >= patience:\n",
    "        print(\"🛑 Early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c510d0e6-b308-4b8b-8e4b-483f80a96349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalStenosisNet(\n",
       "  (cnn): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (img_head): Sequential(\n",
       "    (0): Linear(in_features=1280, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.6, inplace=False)\n",
       "  )\n",
       "  (tabular): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.6, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\"best_multimodal_focal_regularized.pth\", map_location=device)\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5ccaa3e6-9cf6-4f75-9a1b-09cea49cb001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, tabs, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        tabs = tabs.to(device)\n",
    "\n",
    "        outputs = model(images, tabs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "dce387ba-ba10-4ec0-8c57-346c6a80ad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Test Accuracy: 0.5330\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"🎯 Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "898d842e-f3a4-4591-a575-a01f849a09fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚖ Balanced Accuracy: 0.3594\n"
     ]
    }
   ],
   "source": [
    "bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "print(f\"⚖ Balanced Accuracy: {bal_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "58172246-4276-42a9-bdf2-a9929cd30ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[189  71  45]\n",
      " [ 28  14  10]\n",
      " [ 20  10   7]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "64f3b9ac-bafb-4d5f-ba6f-eecf79f8372d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAHUCAYAAABIykBjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVDpJREFUeJzt3XdYFFfbBvB7aUtTlA4RUEHsBTTWKKBi7yZii6hYEkusUUliQKOiGFvsQbFiS2wxGrvYO/aKClaIigqCNOF8f/i5b1ZQWVwYYO7fe811uWfOzDy7k5d99jxnZhRCCAEiIiKSJR2pAyAiIiLpMBEgIiKSMSYCREREMsZEgIiISMaYCBAREckYEwEiIiIZYyJAREQkY0wEiIiIZIyJABERkYwxEaA8oVAocrSEh4d/8rFevXqFwMBAjff177//Yty4cahatSpMTU1haGiIcuXKYdiwYYiMjPzkuD7k2bNn6Nq1K6ytraFQKNChQwetH8PT0xOenp5a3+/HREdHq85vYGBgtn369u2r6pMbO3bseO++P+RDMRHJlYK3GKa8cOLECbXXv/zyCw4cOID9+/ertVeqVAnFixf/pGM9ffoUVlZWCAgIyPEf+VOnTqFNmzYQQmDIkCGoV68eDAwMcOPGDaxevRqXL1/G8+fPPymuDxkxYgQWLFiA0NBQODs7w9zcHK6urlo9xtWrVwG8+YzzU3R0NMqUKYNixYrB3Nwcd+7cgY7O/35zJCYmws7ODjo6OkhISEBu/gQNGTIE8+fP13jbEydOoFSpUihVqpTGxyQqqvSkDoCKprp166q9trKygo6OTpZ2KSQkJKB9+/YwNDTEsWPH1L4UPD09MXDgQPz55595GsPly5fh7OyMHj165Nkx8jsBeJePjw+WLFmCffv2wdvbW9W+fv16ZGRkoEOHDli9enWexyGEQEpKCoyMjArEf39EBQ1LAySZtLQ0TJo0CRUqVIBSqYSVlRX69OmDJ0+eqPXbv38/PD09YWFhASMjIzg6OqJz58549eoVoqOjYWVlBQCYMGGCari5d+/e7z1uSEgIYmNjERwc/N5fhl9++aXa67/++gv16tWDsbExihUrBm9vbxw/flytT2BgIBQKBa5cuYJu3brBzMwMNjY26Nu3L+Lj4wH8b9h87969uHbtmlqJJDw8PNtyydttli9frmq7c+cOunbtCnt7eyiVStjY2KBJkyY4f/68qk92pYFnz55h0KBB+Oyzz2BgYICyZcvixx9/RGpqqlo/hUKBIUOGYNWqVahYsSKMjY1RvXp1/P333+/9XN9Vvnx51K9fH6GhoWrtoaGh6NSpE8zMzLJss379ejRr1gx2dnYwMjJCxYoVMW7cOCQlJan69O7dG/Pnz1fF+XaJjo5Wi33RokWoWLEilEolVqxYoVr3dtRICIFWrVrBwsIC9+7dU+3/1atXqFy5MipWrKh2XKKiiiMCJInMzEy0b98ehw8fxpgxY1C/fn3cvXsXAQEB8PT0xJkzZ2BkZITo6Gi0bt0aDRs2RGhoKEqUKIGHDx9i586dSEtLg52dHXbu3IkWLVrAz88P/fr1AwBVcpCd3bt3Q1dXF23bts1RrGvWrEGPHj3QrFkzrF27FqmpqQgODoanpyf27duHL774Qq1/586d4ePjAz8/P1y6dAn+/v4A3nwB2tnZ4fjx4xg0aBDi4+MRFhYG4M2v94iIiBx/fq1atUJGRgaCg4Ph6OiIp0+f4tixY3jx4sV7t0lJSYGXlxdu376NCRMmoFq1ajh8+DCCgoJw/vx5bN++Xa3/9u3bcfr0aUycOBGmpqYIDg5Gx44dcePGDZQtWzZHcfr5+WHw4MF4/vw5SpYsiRs3buDYsWOYNGkSNm7cmKV/ZGQkWrVqheHDh8PExATXr1/HtGnTcOrUKVVZafz48UhKSsKff/6plozZ2dmp/r1lyxYcPnwYP//8M2xtbWFtbZ3lWAqFAqtWrUKNGjXQpUsXHD58GPr6+hg0aBCioqJw8uRJmJiY5Oh9EhVqgigf+Pr6ChMTE9XrtWvXCgBi48aNav1Onz4tAIgFCxYIIYT4888/BQBx/vz59+77yZMnAoAICAjIUSwVKlQQtra2OeqbkZEh7O3tRdWqVUVGRoaq/eXLl8La2lrUr19f1RYQECAAiODgYLV9DBo0SBgaGorMzExVm4eHh6hcubJavwMHDggA4sCBA2rtUVFRAoBYtmyZEEKIp0+fCgBi9uzZH4zdw8NDeHh4qF4vWrRIABAbNmxQ6zdt2jQBQOzevVvVBkDY2NiIhIQEVVtsbKzQ0dERQUFBHzzu23inT58uXr58KUxNTcW8efOEEEJ8//33okyZMiIzM1MMHjxYfOhPUGZmpkhPTxcHDx4UAMSFCxdU6z60LQBhZmYmnj17lu26d/87OXLkiNDT0xPDhw8XoaGhAoBYsmTJB98jUVHC0gBJ4u+//0aJEiXQtm1bvH79WrXUqFEDtra2quHxGjVqwMDAAAMGDMCKFStw586dfI3zxo0bePToEb7++mu1CW+mpqbo3LkzTpw4gVevXqlt065dO7XX1apVQ0pKCh4/fqyVmMzNzeHs7Izp06dj5syZOHfuHDIzMz+63f79+2FiYpKl7PG2jLJv3z61di8vLxQrVkz12sbGBtbW1rh7926OYzU1NcVXX32F0NBQvH79GitXrkSfPn3ee7XAnTt30L17d9ja2kJXVxf6+vrw8PAAAFy7di3Hx23cuDFKliyZo74NGjTA5MmTMXv2bHz77bfo2bMn/Pz8cnwsosKOiQBJ4t9//8WLFy9gYGAAfX19tSU2NhZPnz4FADg7O2Pv3r2wtrbG4MGD4ezsDGdnZ8yZMyfXx3Z0dMSTJ09yVP+Ni4sDoD7s/Ja9vT0yMzOzXF1gYWGh9lqpVAIAkpOTcxuyGoVCgX379qF58+YIDg6Gu7s7rKys8N133+Hly5fv3S4uLg62trZZvoStra2hp6eneq/vex/Am/ei6fvw8/NDREQEJk+ejCdPnrx3/kZiYiIaNmyIkydPYtKkSQgPD8fp06exadMmAJp9ftmdrw/p0aMHDAwMkJqaiu+//16jbYkKO84RIElYWlrCwsICO3fuzHb9f3+JNmzYEA0bNkRGRgbOnDmDuXPnYvjw4bCxsUHXrl01Pnbz5s2xe/dubNu27aPbv/0yjImJybLu0aNH0NHRyfEvz48xNDQEgCwT994mRf/l5OSEpUuXAgBu3ryJDRs2IDAwEGlpaVi0aFG2+7ewsMDJkychhFBLBh4/fozXr1/D0tJSK+/jXQ0aNED58uUxceJEeHt7w8HBIdt++/fvx6NHjxAeHq4aBQDwwXkP76PJ/QkyMjLQo0cPlCxZEkqlEn5+fjh69CgMDAw0Pi5RYcQRAZJEmzZtEBcXh4yMDNSqVSvLUr58+Szb6Orqok6dOqoZ428n12n6i9vPzw+2trYYM2YMHj58mG2ft79Cy5cvj88++wxr1qxRu2Y9KSkJGzduVF1JoA2lS5cGAFy8eFGt/a+//vrgdq6urvjpp59QtWrVD044bNKkCRITE7Flyxa19pUrV6rW55WffvoJbdu2xahRo97b5+2X99vz+dbixYuz9NXmKEtAQAAOHz6MsLAwrF+/HhcuXOCoAMkKRwRIEl27dkVYWBhatWqFYcOGoXbt2tDX18eDBw9w4MABtG/fHh07dsSiRYuwf/9+tG7dGo6OjkhJSVFdjta0aVMAb0YPnJycsHXrVjRp0gTm5uawtLRUfbG+y8zMDFu3bkWbNm3g5uamdkOhyMhIrF69GhcuXECnTp2go6OD4OBg9OjRA23atMHAgQORmpqK6dOn48WLF5g6darWPhNbW1s0bdoUQUFBKFmyJJycnLBv3z5VUvLWxYsXMWTIEHz11VcoV64cDAwMsH//fly8eBHjxo177/579eqF+fPnw9fXF9HR0ahatSqOHDmCKVOmoFWrVqrPMy/07NkTPXv2/GCf+vXro2TJkvjmm28QEBAAfX19hIWF4cKFC1n6Vq1aFQAwbdo0tGzZErq6uqhWrZrGv+L37NmDoKAgjB8/XpUIBQUFYfTo0fD09ETHjh012h9RoST1bEWSh3evGhBCiPT0dPHrr7+K6tWrC0NDQ2FqaioqVKggBg4cKCIjI4UQQhw/flx07NhRODk5CaVSKSwsLISHh4f466+/1Pa1d+9e4ebmJpRKpQAgfH19PxpTbGysGDt2rKhcubIwNjYWSqVSuLi4iIEDB4pLly6p9d2yZYuoU6eOMDQ0FCYmJqJJkybi6NGjan3eXjXw5MkTtfZly5YJACIqKkrVlt1VA0IIERMTI7788kthbm4uzMzMRM+ePcWZM2fUrhr4999/Re/evUWFChWEiYmJMDU1FdWqVROzZs0Sr1+/VjvGf68aEEKIuLg48c033wg7Ozuhp6cnnJychL+/v0hJSVHrB0AMHjw4S3xOTk4f/Wz/e9XAh2Q38//YsWOiXr16wtjYWFhZWYl+/fqJiIgItfcvhBCpqamiX79+wsrKSigUCrXP932xv1339qqBR48eCWtra9G4cWO1K0IyMzNF27ZtRYkSJdTOGVFRxVsMExERyRjnCBAREckYEwEiIiIZYyJAREQkY0wEiIiIZIyJABERkYwxESAiIpIxJgJEREQyViTvLGjkNkTqECgfLV36/rvpUdFT0cJM6hAoH7k5Fft4p0+gze+L5HPztLav/FQkEwEiIqIcUXBgnJ8AERGRjHFEgIiI5EuDR1YXVUwEiIhIvlgaYGmAiIhIzjgiQERE8sXSABMBIiKSMZYGWBogIiKSM44IEBGRfLE0wESAiIhkjKUBlgaIiIjkjCMCREQkXywNMBEgIiIZY2mApQEiIiI544gAERHJF0sDTASIiEjGWBpgaYCIiEjOOCJARETyxdIAEwEiIpIxlgZYGiAiIpIzjggQEZF8cUSAiQAREcmYDucIMBUiIiKSMY4IEBGRfLE0wBEBIiKSMYVCe4sGDh06hLZt28Le3h4KhQJbtmx5JyxFtsv06dNVfTw9PbOs79q1q8YfARMBIiKifJaUlITq1atj3rx52a6PiYlRW0JDQ6FQKNC5c2e1fv3791frt3jxYo1jYWmAiIjkS6LSQMuWLdGyZcv3rre1tVV7vXXrVnh5eaFs2bJq7cbGxln6aoojAkREJF9aLA2kpqYiISFBbUlNTf3kEP/9919s374dfn5+WdaFhYXB0tISlStXxujRo/Hy5UuN989EgIiISAuCgoJgZmamtgQFBX3yflesWIFixYqhU6dOau09evTA2rVrER4ejvHjx2Pjxo1Z+uQESwNERCRfWiwN+Pv7Y+TIkWptSqXyk/cbGhqKHj16wNDQUK29f//+qn9XqVIF5cqVQ61atRAREQF3d/cc75+JABERyZcWHzqkVCq18sX/X4cPH8aNGzewfv36j/Z1d3eHvr4+IiMjNUoEWBogIiIqoJYuXYqaNWuievXqH+175coVpKenw87OTqNjcESAiIjkS6KrBhITE3Hr1i3V66ioKJw/fx7m5uZwdHQEACQkJOCPP/7AjBkzsmx/+/ZthIWFoVWrVrC0tMTVq1cxatQouLm5oUGDBhrFwkSAiIjkS4ulAU2cOXMGXl5eqtdv5xb4+vpi+fLlAIB169ZBCIFu3bpl2d7AwAD79u3DnDlzkJiYCAcHB7Ru3RoBAQHQ1dXVKBaFEELk/q0UTEZuQ6QOgfLR0qXjpA6B8lFFCzOpQ6B85OZULE/3b9Ryltb2lfzPCK3tKz9xRICIiOSLzxpgIkBERDImUWmgIGEqREREJGMcESAiIvliaYCJABERyRgTAZYGiIiI5IwjAkREJF+cLMhEgIiIZIylAZYGiIiI5IwjAkREJF8sDTARICIiGWNpgKUBIiIiOeOIABERyRdLA0wEiIhIvhRMBFgaICIikjOOCBARkWxxRICJABERyRnzAJYGiIiI5IwjAkREJFssDTARICIiGWMiwNIAERGRrHFEgIiIZIsjAgUwERBCAODJeauBuzNG9GoK90qOsLMyQ5cRv2Nb+EXVehMjA0z6rj3aelWDuZkJ7j56hgXrwhHyxxFVnzKlLDF1REfUcysLpb4e9hy7hpHT/sDjZy+leEukgdlDuyP+6b9Z2mt5t0PrvsNw7dRhnN33Nx7duYnkxAQMDFoM29IuEkRK2rZl7TKsWzYfLTt2g++3owAAC6YH4tCev9X6uVSogkm/LZcgwqKB3zUFKBFYuXIlpk+fjsjISACAq6srvv/+e3z99dcSRyYtEyMlLt18iFV/ncC6Gf2zrA8e3RketVzR58eVuPsoDk3rVcQc/y6IeRKPv8MvwdjQAH8vGIxLNx+i5YC5AICAQa2xcc5ANOo1Q5V4UcHUf/ICiMxM1evH96OwasoYVK7rAQBIS02Bg2tlVKrTCNtCZkoVJmnZ7RtXsG/HZjiWLZdlXfVa9fHt6J9Vr/X09PMzNCqCCkQiMHPmTIwfPx5DhgxBgwYNIITA0aNH8c033+Dp06cYMWKE1CFKZvfRq9h99Op719epVgar/z6Jw2ffJFChm47Cr3MDuFdyxN/hl1CvRlk42VugbrdpeJmUAgAYELAaMYemw7O2Kw6cvJEv74Nyx6R4CbXXR7auRUkbezhVrA4AqN7QGwDw4klsfodGeSQl+RXmTh2PASN+xKY1S7Os19fXRwlzSwkiK6I4IFAwJgvOnTsXCxcuxLRp09CuXTu0b98ewcHBWLBgAX777TepwyvQjp2/gzYeVWFvZQYAaFSrHMo5WWPvsWsAAKWBHoQQSE17rdomJe01MjIyUb+GsyQxU+5kvE7HxSN74ebZgsOZRVjo3Glwq90AVd3rZLv+6sWzGPCVN4b36YTfZ01C/PNn+Rxh0aJQKLS2FFYFYkQgJiYG9evXz9Jev359xMTESBBR4TFq2h9Y8HN33N49GenpGcgUmfh24hocO38HAHDqUjSSktMweVh7/DzvLyigwORh7aGrqwNby+ISR0+auH76KFJeJaJGo+ZSh0J55NiBXYi6dR2T563Mdn2Nz+ujbqOmsLK2xePYR9iwYhF+GfMNguavhr6BQT5HS0VFgUgEXFxcsGHDBvzwww9q7evXr0e5cllrZP+VmpqK1NRUtTaRmQGFjq7W4yyIBnfzRO2qpdF52CLci3mGL9xdMMffB7FPE3Dg5A08fZ6IHmOW4rcffDComwcyMwU27DyLiKv3kPGf2jMVfOfC/0G5GrVRjMPCRdLTx7FYsXAGfgiaBwMDZbZ96ns2U/3boYwLyrpWwpCv2+DcqSOo/UXj/Aq1SCnMv+S1pUAkAhMmTICPjw8OHTqEBg0aQKFQ4MiRI9i3bx82bNjwwW2DgoIwYcIEtTZdm8+hb1c7L0MuEAyV+pgwtC18RoZg55ErAIDLkY9QrXwpDP+6iar+v+/EdVRuNwEWJUzw+nUm4hOTEbVnCu4+jJMyfNLAiyf/4s6lCHQZGSh1KJRHoiKvI/7FM/gP/t8E6czMDFy/dA67tm7A6u3HoKOr/gOnpIUlrKztEPPwXn6HW2QwESggiUDnzp1x8uRJzJw5E1u2bIEQApUqVcKpU6fg5ub2wW39/f0xcuRItTbrhmPzMtwCQ19PFwb6esh8Z+Z/RkYmdHSy/scd9yIJAODxuSuszU3x98FL+RInfbrzB3fCxKwEXN3qSh0K5ZEqbp9j+uJ1am0LZ0yEvYMT2nfxzZIEAMDLhBeIe/IvSnKUiD5BgUgEAKBmzZoICwvTeDulUgmlUn0YrSiVBUyMDODsYKV6XfozC1Rz/QzPE17hfuxzHDoTiSnDOyA5JR33Yp6hYU0X9GhTG2NnblJt83W7urgRFYsnzxNRp1oZ/Pr9l5gbdgCRdx9L8ZZIQyIzE+cP7kT1Rs2yfBkkJyYg/uljvHz+ZnTnacx9AIBpCXOYljDP91gp94yMTeBQRv0eEEpDQxQrXgIOZVyQkvwKf6z6HXW+aIwS5pZ48u8jrFu2AMXMSuDzBl4SRV34cURA4kRAR0fnoydBoVDg9evXH+xTlLlXcsLuJcNUr4NHdwYArPrrBAYErEavcaGYOLQ9lk/xRcnixrgX8wyB8/9Wu6GQa2lrTBzaDuZmxrj76BmCl+7Cb6v35/t7ody5czkC8U8fw82zRZZ1N84ew9ZF01WvN/42CQDg0bkXPL/0zbcYKe/p6OjgftQtHN6zHUlJL1HS3BKVqtfCsB+mwMjYROrwCi/mAVAICe8os3Xr1veuO3bsGObOnQshBJKTkzXar5HbkE8NjQqRpUvHSR0C5aOKFmZSh0D5yM2pWJ7u38J3rdb2Fbeim9b2lZ8kHRFo3759lrbr16/D398f27ZtQ48ePfDLL79IEBkREckBSwMF5IZCAPDo0SP0798f1apVw+vXr3H+/HmsWLECjo6OUodGRERFFG8oVAASgfj4eIwdOxYuLi64cuUK9u3bh23btqFKlSpSh0ZERFTkSVoaCA4OxrRp02Bra4u1a9dmWyogIiLKK4X5l7y2SJoIjBs3DkZGRnBxccGKFSuwYsWKbPtt2rQp23YiIqJPwjxA2kSgV69ezMaIiIgkJGkisHz5cikPT0REMscfowXozoJERET5jYlAAbhqgIiIiKTDRICIiGRLqvsIHDp0CG3btoW9vT0UCgW2bNmitr53795Z9l+3rvpDx1JTUzF06FBYWlrCxMQE7dq1w4MHDzT+DJgIEBGRbEmVCCQlJaF69eqYN2/ee/u0aNECMTExqmXHjh1q64cPH47Nmzdj3bp1OHLkCBITE9GmTRtkZGRoFAvnCBAREeWzli1bomXLlh/so1QqYWtrm+26+Ph4LF26FKtWrULTpk0BAKtXr4aDgwP27t2L5s2b5zgWjggQEZF8KbS3pKamIiEhQW1JTU3NdWjh4eGwtraGq6sr+vfvj8eP//fo+LNnzyI9PR3NmjVTtdnb26NKlSo4duyYRsdhIkBERLKlzdJAUFAQzMzM1JagoKBcxdWyZUuEhYVh//79mDFjBk6fPo3GjRurEovY2FgYGBigZMmSatvZ2NggNjZWo2OxNEBERKQF/v7+GDlypFqbUqnM1b58fHxU/65SpQpq1aoFJycnbN++HZ06dXrvdkIIjecrMBEgIiLZ0uZ9BJRKZa6/+D/Gzs4OTk5OiIyMBADY2toiLS0Nz58/VxsVePz4MerXr6/RvlkaICIi2SosjyGOi4vD/fv3YWdnBwCoWbMm9PX1sWfPHlWfmJgYXL58WeNEgCMCRERE+SwxMRG3bt1SvY6KisL58+dhbm4Oc3NzBAYGonPnzrCzs0N0dDR++OEHWFpaomPHjgAAMzMz+Pn5YdSoUbCwsIC5uTlGjx6NqlWrqq4iyCkmAkREJF8S3WH4zJkz8PLyUr1+O7fA19cXCxcuxKVLl7By5Uq8ePECdnZ28PLywvr161GsWDHVNrNmzYKenh66dOmC5ORkNGnSBMuXL4eurq5GsTARICIi2ZLqWQOenp4QQrx3/a5duz66D0NDQ8ydOxdz5879pFg4R4CIiEjGOCJARESyxacPMhEgIiIZYyLA0gAREZGscUSAiIhkiyMCTASIiEjOmAewNEBERCRnHBEgIiLZYmmAiQAREckYEwGWBoiIiGSNIwJERCRbHBBgIkBERDLG0gBLA0RERLLGEQEiIpItDggwESAiIhljaYClASIiIlnjiAAREckWBwSYCBARkYzp6DATYGmAiIhIxjgiQEREssXSAEcEiIiIZI0jAkREJFu8fJCJABERyRjzAJYGiIiIZI0jAkREJFssDTARICIiGWMiwNIAERGRrHFEgIiIZIsDAkwEiIhIxlgaYGmAiIhI1jgiQEREssUBASYCREQkYywNsDRAREQkaxwRICIi2eKAABMBIiKSMZYGWBogIiKSNY4IEBGRbHFAgIkAERHJGEsDLA0QERHJWpEcEbi+d4bUIVA+KmGsL3UIlI/0dPkLjrSHAwJFNBEgIiLKCZYGWBogIiKSNSYCREQkWwqF9hZNHDp0CG3btoW9vT0UCgW2bNmiWpeeno6xY8eiatWqMDExgb29PXr16oVHjx6p7cPT0xMKhUJt6dq1q8afARMBIiKSrXe/SD9l0URSUhKqV6+OefPmZVn36tUrREREYPz48YiIiMCmTZtw8+ZNtGvXLkvf/v37IyYmRrUsXrxY48+AcwSIiIjyWcuWLdGyZcts15mZmWHPnj1qbXPnzkXt2rVx7949ODo6qtqNjY1ha2v7SbFwRICIiGRLm6WB1NRUJCQkqC2pqalaiTM+Ph4KhQIlSpRQaw8LC4OlpSUqV66M0aNH4+XLlxrvm4kAERHJljZLA0FBQTAzM1NbgoKCPjnGlJQUjBs3Dt27d0fx4sVV7T169MDatWsRHh6O8ePHY+PGjejUqZPG+2dpgIiISAv8/f0xcuRItTalUvlJ+0xPT0fXrl2RmZmJBQsWqK3r37+/6t9VqlRBuXLlUKtWLURERMDd3T3Hx2AiQEREsqXN+wgolcpP/uL/r/T0dHTp0gVRUVHYv3+/2mhAdtzd3aGvr4/IyEgmAkRERDlRUO8n9DYJiIyMxIEDB2BhYfHRba5cuYL09HTY2dlpdCwmAkRERPksMTERt27dUr2OiorC+fPnYW5uDnt7e3z55ZeIiIjA33//jYyMDMTGxgIAzM3NYWBggNu3byMsLAytWrWCpaUlrl69ilGjRsHNzQ0NGjTQKBaFEEJo9d0VAHfjtDNLkwoHPmtAXvisAXkxMcjb8+05+5jW9hU+vH7O+4aHw8vLK0u7r68vAgMDUaZMmWy3O3DgADw9PXH//n307NkTly9fRmJiIhwcHNC6dWsEBATA3Nxco7g5IkBERLIlVWnA09MTH/od/rHf6A4ODjh48KBWYuHlg0RERDLGEQEiIpItPn2QiQAREckY8wCWBoiIiGSNIwJERCRbOhwSYCJARETyxTyApQEiIiJZ44gAERHJFq8aYCJAREQypsM8gKUBIiIiOeOIABERyRZLA0wEiIhIxpgHsDRAREQkaxwRICIi2VKAQwJMBIiISLZ41QBLA0RERLLGEQEiIpItXjXARICIiGSMeQBLA0RERLLGEQEiIpItPoaYiQAREckY84ACVBp48eIFlixZAn9/fzx79gwAEBERgYcPH0ocGRERUdFVIEYELl68iKZNm8LMzAzR0dHo378/zM3NsXnzZty9excrV66UOkQiIiqCeNVAARkRGDlyJHr37o3IyEgYGhqq2lu2bIlDhw5JGBkRERVlCoX2lsKqQCQCp0+fxsCBA7O0f/bZZ4iNjZUgIiIiInkoEKUBQ0NDJCQkZGm/ceMGrKysJIiIiIjkgFcNFJARgfbt22PixIlIT08H8KZmc+/ePYwbNw6dO3eWODoiIiqqFFpcCqsCkQj8+uuvePLkCaytrZGcnAwPDw+4uLigWLFimDx5stThERERFVkFojRQvHhxHDlyBPv370dERAQyMzPh7u6Opk2bSh0aEREVYbxqoIAkAitXroSPjw8aN26Mxo0bq9rT0tKwbt069OrVS8LoiIioqOJjiAtIaaBPnz6Ij4/P0v7y5Uv06dNHgoiIiIjkoUCMCAghsh2eefDgAczMzCSIiIiI5IClAYkTATc3NygUCigUCjRp0gR6ev8LJyMjA1FRUWjRooWEERIRUVHGPEDiRKBDhw4AgPPnz6N58+YwNTVVrTMwMEDp0qV5+SAREVEekjQRCAgIAACULl0aPj4+arcXJiIiymssDRSQOQK+vr5Sh0BERDLEqwYKSCKQkZGBWbNmYcOGDbh37x7S0tLU1r99LDERERFpV4G4fHDChAmYOXMmunTpgvj4eIwcORKdOnWCjo4OAgMDpQ6PiIiKqLcT1rWxFFa5SgRWrVqFBg0awN7eHnfv3gUAzJ49G1u3bs1VEGFhYQgJCcHo0aOhp6eHbt26YcmSJfj5559x4sSJXO2TiIjoY/isgVwkAgsXLsTIkSPRqlUrvHjxAhkZGQCAEiVKYPbs2bkKIjY2FlWrVgUAmJqaqm4u1KZNG2zfvj1X+yQiIqKP0zgRmDt3LkJCQvDjjz9CV1dX1V6rVi1cunQpV0GUKlUKMTExAAAXFxfs3r0bAHD69Gkolcpc7ZOIiOhjdBQKrS2FlcaJQFRUFNzc3LK0K5VKJCUl5SqIjh07Yt++fQCAYcOGYfz48ShXrhx69eqFvn375mqfREREH6NQaG8prDROBMqUKYPz589naf/nn39QqVKlXAUxdepU/PDDDwCAL7/8EkeOHMG3336LP/74A1OnTs3VPomIiAqqQ4cOoW3btrC3t4dCocCWLVvU1gshEBgYCHt7exgZGcHT0xNXrlxR65OamoqhQ4fC0tISJiYmaNeuHR48eKBxLBonAt9//z0GDx6M9evXQwiBU6dOYfLkyfjhhx/w/fffaxxAeno6+vTpgzt37qja6tSpg5EjR6Jdu3Ya74+IiCinpLpqICkpCdWrV8e8efOyXR8cHIyZM2di3rx5OH36NGxtbeHt7Y2XL1+q+gwfPhybN2/GunXrcOTIESQmJqJNmzaquXs5/gyEEEKjLQCEhIRg0qRJuH//PgDgs88+Q2BgIPz8/DTdFYA3Ew0jIiJQtmzZXG3/rrtxqVrZDxUOJYz1pQ6B8pGebiEegyWNmRjk7fke+OeVj3fKocVfVs7VdgqFAps3b1bddl8IAXt7ewwfPhxjx44F8ObXv42NDaZNm4aBAwciPj4eVlZWWLVqFXx8fAAAjx49goODA3bs2IHmzZvn+Pi5unywf//+uHv3Lh4/fozY2Fjcv38/10kA8GaOwLvDIpTV2pVLMKRvN7RvWhdftfJAwNhhuH83Sq1P8qtXmDdjCrq3b4o2np/Dr1t7bNu0XqKI6VNFnD2Nkd99i1bejVC7RkWE79/73r5BvwSgdo2KWLt6RT5GSNp09sxpDBvyDZo1bgj3qhVwYJ/6+RZCYNGCuWjWuCHq1aqO/n2+xu1bkRJFS+9KTU1FQkKC2pKaqvkP06ioKMTGxqJZs2aqNqVSCQ8PDxw7dgwAcPbsWaSnp6v1sbe3R5UqVVR9cuqT7ixoaWn5KZuruLi44JdffsGxY8dQs2ZNmJiYqK3/7rvvtHKcwu7SuTNo17krXCtWRkZGBpYvngv/4d8gZM1mGBkZAwAWzQnGhYjTGBsQBBs7e5w9eRxzZ0yGhaU16jfykvgdkKZSkpNRzrU82rbviLGjhr23X/j+vbh86SKsrKzzMTrStpTkZLi6VkC7Dp3w/Yisf/dWhC5B2MrlCJwUBCen0ljy+yJ8O6AvNm/7ByYmptnskT5Gm7P9g4KCMGHCBLW2gIAAjW+MFxsbCwCwsbFRa7exsVHduyc2NhYGBgYoWbJklj5vt88pjROBMmXKfLAW8t9af04tWbIEJUqUwNmzZ3H27Fm1dQqFgonA/5sya5Ha61E/TkSX1p6IvH4V1dxqAQCuXr6Apq3aobr75wCA1h2+xPatf+Dm9StMBAqh+l80Qv0vGn2wz+N//8WvUydhzoIQjBz6TT5FRnmhQcNGaNAw+/MthMCa1Svh1/8bNGn65lfgxMlT0dSzAf7Z/je+7NI1P0MtMrQ529/f3x8jR45Ua/uUS+Df/a4VQnx0LkJO+rxL40Rg+PDhaq/T09Nx7tw57Ny5M1eTBYE3wyCkuaSkRABAseJmqrYq1d1x4nA4WrTpAAtLa1yIOI2H9++i1vCxEkVJeSkzMxMBP41FT9++cHYpJ3U4lIcePniAp0+foG79Bqo2AwMD1Kz5OS5eOMdEoABQKpVaufeNra0tgDe/+u3s7FTtjx8/Vo0S2NraIi0tDc+fP1cbFXj8+DHq16+v0fE0TgSGDct+eHL+/Pk4c+aMprtTk5aWhqioKDg7O0NPL2ehpaamZqnBpKZ+WhZWGAghsPi36ahS3Q1lnP/3BTBoxDjMmhqI7u29oaurBx0dBUaMC0SV6u4SRkt5ZeWyJdDT1YVP96+lDoXyWFzcEwCAhYWFWru5hQViYh5JEVKRUBCfEVCmTBnY2tpiz549qvv2pKWl4eDBg5g2bRoAoGbNmtDX18eePXvQpUsXAEBMTAwuX76M4OBgjY6ntYcOtWzZEhs3bszVtq9evYKfnx+MjY1RuXJl3Lt3D8CbuQEfu49AUFAQzMzM1JYFszX7EAqjeTOmIOpWJPwnTFNr3/JHGK5fuYgJwb9h/rJ1GDB0NObOmIyI03xmQ1Fz7eoVrFuzCj9PDCqQf8woj2Rzrnn+c09Hi4smEhMTcf78edV9eaKionD+/Hncu3cPCoUCw4cPx5QpU7B582ZcvnwZvXv3hrGxMbp37w4AMDMzg5+fH0aNGoV9+/bh3Llz6NmzJ6pWrYqmTZtqFIvWHkP8559/wtzcPFfb+vv748KFCwgPD0eLFi1U7U2bNkVAQADGjRv3wW3frcnEJuYqjEJj/swgHD8SjhkLlsHK2lbVnpqagmWLfkNA0GzUafCmzljWxRW3I6/jzzXL4f55XalCpjxwPuIMnj+LQ7uWjVVtGRkZmDMzGOvCVmLrP/skjI60zcLCCgAQ9/Sp2qTQZ3FxWUYJqOA7c+YMvLz+N2/r7feYr68vli9fjjFjxiA5ORmDBg3C8+fPUadOHezevRvFihVTbTNr1izo6emhS5cuSE5ORpMmTbB8+XK12//nhMaJgJubm1r2KYRAbGwsnjx5ggULFmi6OwDAli1bsH79etStW1dt35UqVcLt27c/uG12NZnn6UXzPgJCCMyfGYSjB/fj1/lLYWdfSm3969ev8fr1ayh01H8d6OjoIjNT49tFUAHXsk071K5bT63tu2/7o2WbdmjbvpNEUVFe+axUKVhaWuHE8WOoUPHNXVzT09Nw9uxpfDd8lMTRFV5SjaZ4enriQ7fxUSgUCAwM/OAVB4aGhpg7dy7mzp37SbFonAi8veHBWzo6OrCysoKnpycqVKiQqyCePHkCa+uslz0lJSVxyOs/5v46GQf2/IMJ0+bAyNgEz+KeAgBMTE2hVBrCxMQU1dxqIWTeTCiVhrC2tcOlc2ex959tGPjdaImjp9x49SoJD/6/VAYAjx4+wM3r11DczAy2dvYoUUL90iE9PT1YWFjCqXSZ/A6VtODVqyTc/8/5fvjwAW78//m2s7NH9569ELpkMRydnODo6ITQkMUwNDREy9ZtJIy6cNPhV4xmicDr169RunRpNG/eXDWrURs+//xzbN++HUOHDgXwvwwtJCQE9erV+9CmsvL35g0AgNGD1R/ENPrHX9CsdXsAwA8TgxG6cA6mBvrjZUI8rG3t0HvgULTp2CXf46VPd+3KFXzb31f1evaMN3NCWrftgIBfgqQKi/LI1SuXMaDv/873zOlv5ki1bdcBEyZPhW/ffkhJTcHUSRORkBCPKlWrYcHipbyHAH0SjW8xbGxsjGvXrsHJyUlrQRw7dgwtWrRAjx49sHz5cgwcOBBXrlzB8ePHcfDgQdSsWVOj/fEWw/LCWwzLC28xLC95fYvhkX9d19q+ZrbL3ai41DS+aqBOnTo4d+6cVoOoX78+jh49ilevXsHZ2Rm7d++GjY0Njh8/rnESQERElFNSPXSoINF4jsCgQYMwatQoPHjwINvbAVerVi1XgVStWhUrVvAe6URERPkpx4lA3759MXv2bNVTjv5721+FQqG6rWFOH3+YkJCQ4yCLFy+e475EREQ5xcmCGswR0NXVRUxMDJKTkz/YL6dzB3R0dHI8lKLps5U5R0BeOEdAXjhHQF7yeo7AmO03tLav4Nbltbav/JTjEYG3+YK2JgkeOHBA9e/o6GiMGzcOvXv3Vl0lcPz4caxYsQJBQZwZTURElFc0miOgzckQHh4eqn9PnDgRM2fORLdu3VRt7dq1Q9WqVfH777/D19c3u10QERF9Em0+hriw0igRcHV1/Wgy8OzZM42DOH78OBYtWpSlvVatWujXr5/G+yMiIsoJrT1wpxDTKBGYMGECzMzMPt5RQw4ODli0aBFmzJih1r548WI4ODho/XhERET0hkaJQNeuXbO9FfCnmjVrFjp37oxdu3ahbt03D8Y5ceIEbt++nesnGhIREX0MKwMajIrk5c0SWrVqhcjISLRr1w7Pnj1DXFwc2rdvj5s3b6JVq1Z5dlwiIpI3HYVCa0thpfFVA3mlVKlSmDJlSp4eg4iIiNTlOBHIzMzMyzjw4sULLF26FNeuXYNCoUClSpXQt2/fPJmTQEREBLA0ABSQCZNnzpyBs7MzZs2ahWfPnuHp06eYOXMmnJ2dERERIXV4RERUROkotLcUVho/ayAvjBgxAu3atUNISAj09N6E9Pr1a/Tr1w/Dhw/HoUOHJI6QiIioaCoQicCZM2fUkgAA0NPTw5gxY1CrVi0JIyMioqKsME/y05YCURooXrw47t27l6X9/v37KFasmAQRERGRHCgU2lsKqwKRCPj4+MDPzw/r16/H/fv38eDBA6xbtw79+vVTu+0wERERaVeBKA38+uuvUCgU6NWrF16/fg0hBAwMDPDtt99i6tSpUodHRERFVGGe5KctOX4McX549eoVbt++DSEEXFxcYGxsnKv98DHE8sLHEMsLH0MsL3n9GOIp+25rbV8/NHHW2r7yk6QjAn379s1Rv9DQ0DyOhIiISJ4kTQSWL18OJycnuLm55fmdC4mIiN7F0oDEicA333yDdevW4c6dO+jbty969uwJc3NzKUMiIiIZYSIg8VUDCxYsQExMDMaOHYtt27bBwcEBXbp0wa5duzhCQERElA8kv3xQqVSiW7du2LNnD65evYrKlStj0KBBcHJyQmJiotThERFREaZQKLS2FFYF4vLBt95+mEKIPH/IEREREUsDBWBEIDU1FWvXroW3tzfKly+PS5cuYd68ebh37x5MTU2lDo+IiKhIk3REYNCgQVi3bh0cHR3Rp08frFu3DhYWFlKGREREMlKIR/S1RtIbCuno6MDR0RFubm4frK9s2rRJo/3yhkLywhsKyQtvKCQveX1DodmHo7S2r+ENy2htX/lJ0hGBXr16FeoJFkRERIWd5DcUIiIikgonCxawqwaIiIjyEwelC8BVA0RERCQdjggQEZFs6YBDAkwEiIhItlgaYGmAiIhI1jgiQEREssWrBpgIEBGRjOmwNsDSABERkZxxRICIiGSLAwIcESAiIhnTUSi0tmiidOnSUCgUWZbBgwcDAHr37p1lXd26dfPiI+CIABERUX47ffo0MjIyVK8vX74Mb29vfPXVV6q2Fi1aYNmyZarXBgYGeRILEwEiIpItqUoDVlZWaq+nTp0KZ2dneHh4qNqUSiVsbW3zPBaWBoiISLZ0tLikpqYiISFBbUlNTf1oDGlpaVi9ejX69u2r9kTe8PBwWFtbw9XVFf3798fjx4+19r7/i4kAERGRFgQFBcHMzExtCQoK+uh2W7ZswYsXL9C7d29VW8uWLREWFob9+/djxowZOH36NBo3bpyjxEJTCiGE0PpeJXY3TvsfFBVcJYz1pQ6B8pGeLqd5y4mJQd6e7xVn7mttX12rWmf5olYqlVAqlR/crnnz5jAwMMC2bdve2ycmJgZOTk5Yt24dOnXqpJV43+IcASIiki1tphk5+dJ/1927d7F3715s2rTpg/3s7Ozg5OSEyMjITwkxWywNEBERSWTZsmWwtrZG69atP9gvLi4O9+/fh52dndZjYCJARESyJdV9BAAgMzMTy5Ytg6+vL/T0/jdAn5iYiNGjR+P48eOIjo5GeHg42rZtC0tLS3Ts2FGbbx8ASwNERCRjUs442bt3L+7du4e+ffuqtevq6uLSpUtYuXIlXrx4ATs7O3h5eWH9+vUoVqyY1uNgIkBERCSBZs2aIbv5+kZGRti1a1e+xcFEgIiIZIvPGmAiQEREMqZgJsDJgkRERHLGEQEiIpIt/hpmIkBERDLG0gCTISIiIlnjiAAREckWxwOYCBARkYyxNFBEEwEjA1Y85IRPo5MXXR2ebyJtKpKJABERUU7wZyMTASIikjGWBpgMERERyRpHBIiISLY4HsBEgIiIZIyVAZYGiIiIZI0jAkREJFs6LA4wESAiIvliaYClASIiIlnjiAAREcmWgqUBJgJERCRfLA2wNEBERCRrHBEgIiLZ4lUDTASIiEjGWBpgaYCIiEjWOCJARESyxREBJgJERCRjvHyQpQEiIiJZ44gAERHJlg4HBJgIEBGRfLE0wNIAERGRrHFEgIiIZItXDTARICIiGWNpgKUBIiIiWeOIABERyRavGmAiQEREMsbSAEsDREREssYRASIiki1eNcBEgIiIZIx5AEsDREREssYRASIiki0d1gaYCBARkXwxDWBpgIiISNaYCBARkXwptLhoIDAwEAqFQm2xtbVVrRdCIDAwEPb29jAyMoKnpyeuXLnySW/1fZgIEBGRbCm0+D9NVa5cGTExMarl0qVLqnXBwcGYOXMm5s2bh9OnT8PW1hbe3t54+fKlNt8+ACYCREREktDT04Otra1qsbKyAvBmNGD27Nn48ccf0alTJ1SpUgUrVqzAq1evsGbNGq3HwUSAiIhkS6HQ3pKamoqEhAS1JTU19b3HjoyMhL29PcqUKYOuXbvizp07AICoqCjExsaiWbNmqr5KpRIeHh44duyY1j8DJgJERCRb2pwiEBQUBDMzM7UlKCgo2+PWqVMHK1euxK5duxASEoLY2FjUr18fcXFxiI2NBQDY2NiobWNjY6Nap028fJCIiEgL/P39MXLkSLU2pVKZbd+WLVuq/l21alXUq1cPzs7OWLFiBerWrQsAULxzjwMhRJY2beCIABERyZcWhwSUSiWKFy+utrwvEXiXiYkJqlatisjISNXVA+/++n/8+HGWUQJtYCJARESyJeVVA/+VmpqKa9euwc7ODmXKlIGtrS327NmjWp+WloaDBw+ifv36n/qWs2BpgIiIKJ+NHj0abdu2haOjIx4/foxJkyYhISEBvr6+UCgUGD58OKZMmYJy5cqhXLlymDJlCoyNjdG9e3etx1JgEoFbt27h9u3baNSoEYyMjPKsFkJERPSWVF8zDx48QLdu3fD06VNYWVmhbt26OHHiBJycnAAAY8aMQXJyMgYNGoTnz5+jTp062L17N4oVK6b1WBRCCKH1vWogLi4OPj4+2L9/PxQKBSIjI1G2bFn4+fmhRIkSmDFjhsb7fPwyPQ8ipYLKRFlg8lnKB7o6/IEgJ4Z5/H/vs9EJWttXzdLFtbav/CT5HIERI0ZAT08P9+7dg7Gxsardx8cHO3fulDAyIiIq6iS6w3CBIvlPqd27d2PXrl0oVaqUWnu5cuVw9+5diaIiIiJZKMzf4Foi+YhAUlKS2kjAW0+fPs3xZRdERESUO5InAo0aNcLKlStVrxUKBTIzMzF9+nR4eXlJGBkRERV1BeXyQSlJXhqYPn06PD09cebMGaSlpWHMmDG4cuUKnj17hqNHj0odHhERFWG8OK0AjAhUqlQJFy9eRO3ateHt7Y2kpCR06tQJ586dg7Ozs9ThERERFWmSXj6Ynp6OZs2aYfHixXB1ddXafnn5oLzw8kF54eWD8pLXlw9euPdSa/uq7qj9a/zzg6R/QfX19XH58mXeOIiIiKTBrx/pSwO9evXC0qVLpQ6DiIhIliQfU01LS8OSJUuwZ88e1KpVCyYmJmrrZ86cKVFkRERU1BXm2f7aInkicPnyZbi7uwMAbt68qbaOJQMiIspL/JopAInAgQMHpA6BiIhItiSfI/DWrVu3sGvXLiQnJwMAJH4WEhERyQCfNVAAEoG4uDg0adIErq6uaNWqFWJiYgAA/fr1w6hRoySOjoiIijRmAtInAiNGjIC+vj6fPpgDq5aFoH8vHzRrVBttvRvBf9R3uBcdpdZHCIHQxfPRoYUXmjSoiaEDeiPq9i2JIqZPdfbMaQwb8g2aNW4I96oVcGDfXrX1QggsWjAXzRo3RL1a1dG/z9e4fStSomhJ21p6N0b1yuWzLFN+mSB1aFSESJ4I7N69G9OmTePTB3PgfMQZdPyqGxYvW4NZ839HRsZrjBwyAMnJr1R91qwIxfo1KzFizA8IWbEO5haWGDG4P14lJUkYOeVWSnIyXF0rYOwP47NdvyJ0CcJWLsfYH8Zj1do/YGFphW8H9EVSUmI+R0p5IWz9n9gXfkS1LF6yDADg3byFxJEVHXzWQAFIBPj0wZybMXcxWrXtgDLOLnBxrQD/gEn4NzYGN65dBfDm1+GGtavQq88AeDT2RlmXcvhxwhSkpqRgz87tEkdPudGgYSMM/m44mjRtlmWdEAJrVq+EX/9v0KRpM7iUc8XEyVORkpKCf7b/LUG0pG3m5uawtLJSLYfCD8DBwRG1Pq8tdWhFhkKhvaWwkjwR4NMHcy8p8c2vvuLFzQAAMQ8f4FncU3xet76qj4GBAWq418Lli+elCJHy0MMHD/D06RPUrd9A1WZgYICaNT/HxQvnJIyM8kJ6Whq2//0XOnTqzEurSaskv3yQTx/MHSEE5s0MRrUa7ijrUg4AEBf3FABgbmGh1rekhQViYx7le4yUt+LingAALN453+YWFojh+S5y9u/fi5cvX6Jdh45Sh1KkMKUqAInA26cPLly4ELq6uqqnDw4ePBh2dnYf3T41NRWpqanqbWk6Rb6sMCt4Mm7fuon5S1ZmXfnOrwUhBH9BFGXZnFue76Jn88aNaPBFI1hb20gdStHC/6tInwgAgK2tLSZMyN0s2KCgoCzbjh73E77/4WdthFYgzQqegqOHDmDu7ytgbWOrarewsAQAPHv6FJaWVqr2F8+ewdzcIst+qHCzsHhzjuOePoWVlbWq/VlcXJZRAircHj16iJMnjmHmnLlSh0JFkORzBMqUKYPx48fjxo0budre398f8fHxast3o8ZqOcqCQQiBWdMm49CBvZi9MBT2n6lfaWH3WSmYW1ji9Mnjqrb09HScjziDKtVq5HO0lNc+K1UKlpZWOHH8mKotPT0NZ8+eRrXqbhJGRtq2dfMmmJtboGEjT6lDKXJ41UABGBEYOnQo1q5di8mTJ8PNzQ1ff/01fHx8clQWAAClUpmlDJDyMj0vQpXczGmTsHfnDkyZ8RuMjU0Q9/TNnABTU1MoDQ2hUCjQpdvXWL0sBA6Ojijl4IRVy0KgNDSEd4vWEkdPufHqVRLu37unev3w4QPcuH4Nxc3MYGdnj+49eyF0yWI4OjnB0dEJoSGLYWhoiJat20gYNWlTZmYmtm7ehLbtO0BPT/I/2UUOq2iAQhSQe/nevHkTYWFhWLduHe7cuQMvLy/07NkTvXr10nhfj4toItCwVpVs2/0DJqFV2w4A3owaLPt9AbZu+gOJLxNQsUo1jBzzo2pCYVFkoiy6fxzPnD6JAX19s7S3bdcBEyZPhRACixfOw6Y/NiAhIR5VqlbDuB9/hks5VwmizR+6OvL6y33s6BF8O8APW7fvROnSZaQOJ98Z5vH/vW/Evvp4pxwqb5v1UvjCoMAkAv914sQJfPvtt7h48SIyMjI03r6oJgKUvaKcCFBWcksE5C6vE4GbWkwEXAtpIlCg/oKeOnUKa9aswfr16xEfH48vv/xS6pCIiKgoY14pfSLwtiSwZs0aREdHw8vLC1OnTkWnTp1QrFgxqcMjIiIq0iRPBCpUqIBatWph8ODB6Nq1K2xtbT++ERERkRYU5tn+2iJ5InD9+nW4uhbdiU1ERFRw8aqBAnAfAVdXV7x48QJLliyBv78/nj17BgCIiIjAw4cPJY6OiIioaJN8RODixYto0qQJSpQogejoaPTv3x/m5ubYvHkz7t69q/ZAIiIiIm3igEABGBEYMWIE+vTpg8jISBgaGqraW7ZsiUOHDkkYGRERFXkKLS6FlOQjAmfOnMHvv/+epf2zzz5DbGysBBERERHJh+SJgKGhIRISErK037hxA1ZWVtlsQUREpB28aqAAlAbat2+PiRMnIj39zd0AFQoF7t27h3HjxqFz584SR0dEREWZQqG9pbCSPBH49ddf8eTJE1hbWyM5ORkeHh5wdnaGqakpJk+eLHV4RERERVqBedbA/v37ERERgczMTNSsWRNNmjTJ9b74rAF54bMG5IXPGpCXvH7WQPTTFK3tq7Sl4cc7FUCSjQicPHkS//zzj+p148aNYWVlhQULFqBbt24YMGAAUlNTpQqPiIjkgFcNSJcIBAYG4uLFi6rXly5dQv/+/eHt7Y1x48Zh27ZtCAoKkio8IiIiWZAsETh//rza8P+6detQu3ZthISEYOTIkfjtt9+wYcMGqcIjIiIZUGjxf4WVZMXV58+fw8bGRvX64MGDaNGiher1559/jvv370sRGhERyURhnu2vLZKNCNjY2CAqKgoAkJaWhoiICNSrV0+1/uXLl9DX15cqPCIiIlmQLBFo0aIFxo0bh8OHD8Pf3x/GxsZo2LChav3Fixfh7OwsVXhERCQDUs0VDAoKwueff45ixYrB2toaHTp0wI0bN9T69O7dGwqFQm2pW7dubt/qe0mWCEyaNAm6urrw8PBASEgIQkJCYGBgoFofGhqKZs2aSRUeERHJgFQ3FDp48CAGDx6MEydOYM+ePXj9+jWaNWuGpKQktX4tWrRATEyMatmxY4cW3/0bkt9HID4+HqamptDV1VVrf/bsGUxNTdWSg5zifQTkhfcRkBfeR0Be8vo+Ag+ea+8y9VIllbne9u2N9Q4ePIhGjRoBeDMi8OLFC2zZskVLEWZP8jsLmpmZZUkCAMDc3DxXSQAREVHOaa84kJqaioSEBLUlp/fDiY+PB/Dmu++/wsPDYW1tDVdXV/Tv3x+PHz/+xPebleSJABERkVS0WRoICgqCmZmZ2pKT++EIITBy5Eh88cUXqFKliqq9ZcuWCAsLw/79+zFjxgycPn0ajRs31vrN9iQvDeQFlgbkhaUBeWFpQF7yujTw8EWa1vZlaSSyfEkrlUoolR8uGQwePBjbt2/HkSNHUKpUqff2i4mJgZOTE9atW4dOnTppJWagADyGmIiISCraTCtz8qX/rqFDh+Kvv/7CoUOHPpgEAICdnR2cnJwQGRn5KWFmwUSAiIhkS6obCgkhMHToUGzevBnh4eEoU6bMR7eJi4vD/fv3YWdnp9VYOEeAiIgonw0ePBirV6/GmjVrUKxYMcTGxiI2NhbJyckAgMTERIwePRrHjx9HdHQ0wsPD0bZtW1haWqJjx45ajYVzBKjQ4xwBeeEcAXnJ6zkCsfHa+76wNcv53XAV7xmKWLZsGXr37o3k5GR06NAB586dw4sXL2BnZwcvLy/88ssvcHBw0FbIb2JhIkCFHRMBeWEiIC95nggkaDERKF44b4vP0gAREZGM8acUERHJFseXmAgQEZGM8THELA0QERHJGkcEiIhIthQsDjARICIiGWMewNIAERGRnHFEgIiIZIsDAkwEiIhIxnjVAEsDREREssYRASIiki1eNcBEgIiIZIylAZYGiIiIZI2JABERkYyxNEBERLLF0gBHBIiIiGSNIwJERCRbvGqAiQAREckYSwMsDRAREckaRwSIiEi2OCDARICIiOSMmQBLA0RERHLGEQEiIpItXjXARICIiGSMVw2wNEBERCRrHBEgIiLZ4oAAEwEiIpIzZgIsDRAREckZRwSIiEi2eNUAEwEiIpIxXjXA0gAREZGsKYQQQuog6NOlpqYiKCgI/v7+UCqVUodDeYznW154vikvMREoIhISEmBmZob4+HgUL15c6nAoj/F8ywvPN+UllgaIiIhkjIkAERGRjDERICIikjEmAkWEUqlEQEAAJxLJBM+3vPB8U17iZEEiIiIZ44gAERGRjDERICIikjEmAkRERDLGRICoAAsPD4dCocCLFy+kDoWIiigmAoVI7969oVAoMHXqVLX2LVu2QMEnZ0ji7Tn55ptvsqwbNGgQFAoFevfunf+B5UJgYCBq1KghdRiF3uPHjzFw4EA4OjpCqVTC1tYWzZs3x/Hjx6UOjShbTAQKGUNDQ0ybNg3Pnz+XOhT6fw4ODli3bh2Sk5NVbSkpKVi7di0cHR0ljOyNtLQ0qUOQlc6dO+PChQtYsWIFbt68ib/++guenp549uyZZDHxvwH6ECYChUzTpk1ha2uLoKCg9/bZuHEjKleuDKVSidKlS2PGjBn5GKH8uLu7w9HREZs2bVK1bdq0CQ4ODnBzc1O1paam4rvvvoO1tTUMDQ3xxRdf4PTp02r72rFjB1xdXWFkZAQvLy9ER0dnOd6xY8fQqFEjGBkZwcHBAd999x2SkpJU60uXLo1Jkyahd+/eMDMzQ//+/QEAY8eOhaurK4yNjVG2bFmMHz8e6enpAIDly5djwoQJuHDhAhQKBRQKBZYvXw4AiI+Px4ABA2BtbY3ixYujcePGuHDhgrY+viLlxYsXOHLkCKZNmwYvLy84OTmhdu3a8Pf3R+vWrQF8+PO8ceMGFAoFrl+/rrbfmTNnonTp0nh7tffVq1fRqlUrmJqawsbGBl9//TWePn2q6u/p6YkhQ4Zg5MiRsLS0hLe3d462I3liIlDI6OrqYsqUKZg7dy4ePHiQZf3Zs2fRpUsXdO3aFZcuXUJgYCDGjx+v+qNOeaNPnz5YtmyZ6nVoaCj69u2r1mfMmDHYuHEjVqxYgYiICLi4uKB58+aqX4r3799Hp06d0KpVK5w/fx79+vXDuHHj1PZx6dIlNG/eHJ06dcLFixexfv16HDlyBEOGDFHrN336dFSpUgVnz57F+PHjAQDFihXD8uXLcfXqVcyZMwchISGYNWsWAMDHxwejRo1C5cqVERMTg5iYGPj4+EAIgdatWyM2NhY7duzA2bNn4e7ujiZNmkj6C7egMjU1hampKbZs2YLU1NQs6z/2eZYvXx41a9ZEWFiY2nZr1qxB9+7doVAoEBMTAw8PD9SoUQNnzpzBzp078e+//6JLly5q26xYsQJ6eno4evQoFi9enOPtSIYEFRq+vr6iffv2Qggh6tatK/r27SuEEGLz5s3i7ans3r278Pb2Vtvu+++/F5UqVcrXWOXi7Tl58uSJUCqVIioqSkRHRwtDQ0Px5MkT0b59e+Hr6ysSExOFvr6+CAsLU22blpYm7O3tRXBwsBBCCH9/f1GxYkWRmZmp6jN27FgBQDx//lwIIcTXX38tBgwYoBbD4cOHhY6OjkhOThZCCOHk5CQ6dOjw0diDg4NFzZo1Va8DAgJE9erV1frs27dPFC9eXKSkpKi1Ozs7i8WLF3/8A5KhP//8U5QsWVIYGhqK+vXrC39/f3HhwgUhRM4+z5kzZ4qyZcuq1t24cUMAEFeuXBFCCDF+/HjRrFkzte3v378vAIgbN24IIYTw8PAQNWrUUOuTk+1InvQkzUIo16ZNm4bGjRtj1KhRau3Xrl1D+/bt1doaNGiA2bNnIyMjA7q6uvkZpmxYWlqidevWWLFihepXn6WlpWr97du3kZ6ejgYNGqja9PX1Ubt2bVy7dg3Am3NXt25dtYmf9erVUzvO2bNncevWLbVfjEIIZGZmIioqChUrVgQA1KpVK0uMf/75J2bPno1bt24hMTERr1+//ugjbc+ePYvExERYWFiotScnJ+P27dsf+1hkqXPnzmjdujUOHz6M48ePY+fOnQgODsaSJUvw5MmTj36eXbt2xffff48TJ06gbt26CAsLQ40aNVCpUiUAb87JgQMHYGpqmuXYt2/fhqurK4Cs/w3kdDuSHyYChVSjRo3QvHlz/PDDD2qz0oUQWa4gELyLdL7o27evaoh+/vz5auvenoPszs3btpycp8zMTAwcOBDfffddlnX/nZhoYmKitu7EiRPo2rUrJkyYgObNm8PMzAzr1q376PyRzMxM2NnZITw8PMu6EiVKfDReuTI0NIS3tze8vb3x888/o1+/fggICMCgQYM++nna2dnBy8sLa9asQd26dbF27VoMHDhQ1S8zMxNt27bFtGnTsuzDzs5O9e93/xvI6XYkP0wECrGpU6eiRo0aapl8pUqVcOTIEbV+x44dg6urK0cD8liLFi1Us7ObN2+uts7FxQUGBgY4cuQIunfvDgBIT0/HmTNnMHz4cABvzt2WLVvUtjtx4oTaa3d3d1y5cgUuLi4axXb06FE4OTnhxx9/VLXdvXtXrY+BgQEyMjKyHC82NhZ6enooXbq0Rsek/3l7bnP6efbo0QNjx45Ft27dcPv2bXTt2lW1zt3dHRs3bkTp0qWhp5fzP+G53Y5kQMKyBGnov3ME3vr666+FoaGhao7A2bNnhY6Ojpg4caK4ceOGWL58uTAyMhLLli3L/4Bl4N1zEh8fL+Lj41Wv384REEKIYcOGCXt7e/HPP/+IK1euCF9fX1GyZEnx7NkzIYQQd+/eFQYGBmLEiBHi+vXrIiwsTNja2qrNEbhw4YIwMjISgwYNEufOnRM3b94UW7duFUOGDFEd08nJScyaNUstzi1btgg9PT2xdu1acevWLTFnzhxhbm4uzMzMVH3CwsKEiYmJOHfunHjy5IlISUkRmZmZ4osvvhDVq1cXO3fuFFFRUeLo0aPixx9/FKdPn9bqZ1kUPH36VHh5eYlVq1aJCxcuiDt37ogNGzYIGxsb0bdv3xx/nvHx8cLQ0FBUr15dNGnSRO0YDx8+FFZWVuLLL78UJ0+eFLdv3xa7du0Sffr0Ea9fvxZCvJkjMGzYMI23I3liIlCIZJcIREdHC6VSKf6b0/3555+iUqVKQl9fXzg6Oorp06fnc6Tykd05+a//JgLJycli6NChwtLSUiiVStGgQQNx6tQptf7btm0TLi4uQqlUioYNG4rQ0FC1REAIIU6dOiW8vb2FqampMDExEdWqVROTJ09Wrc8uERDizaRRCwsLYWpqKnx8fMSsWbPUEoGUlBTRuXNnUaJECQFAlTwmJCSIoUOHCnt7e6Gvry8cHBxEjx49xL179zT9uIq8lJQUMW7cOOHu7i7MzMyEsbGxKF++vPjpp5/Eq1evhBA5/zy/+uorAUCEhoZmOc7NmzdFx44dRYkSJYSRkZGoUKGCGD58uGqiaXaJQE62I3niY4iJiIhkjPcRICIikjEmAkRERDLGRICIiEjGmAgQERHJGBMBIiIiGWMiQEREJGNMBIiIiGSMiQAREZGMMREgKgQCAwNRo0YN1evevXujQ4cO+R5HdHQ0FAoFzp8/n+/HJqK8wUSA6BP07t0bCoUCCoUC+vr6KFu2LEaPHo2kpKQ8Pe6cOXOwfPnyHPXllzcRfQgfQUX0iVq0aIFly5YhPT0dhw8fRr9+/ZCUlISFCxeq9UtPT4e+vr5WjmlmZqaV/RARcUSA6BMplUrY2trCwcEB3bt3R48ePbBlyxbVcH5oaCjKli0LpVIJIQTi4+MxYMAAWFtbo3jx4mjcuDEuXLigts+pU6fCxsYGxYoVg5+fH1JSUtTWv1sayMzMxLRp0+Di4gKlUglHR0dMnjwZAFCmTBkAgJubGxQKBTw9PVXbLVu2DBUrVoShoSEqVKiABQsWqB3n1KlTcHNzg6GhIWrVqoVz585p8ZMjooKAIwJEWmZkZIT09HQAwK1bt7BhwwZs3LgRurq6AIDWrVvD3NwcO3bsgJmZGRYvXowmTZrg5s2bMDc3x4YNGxAQEID58+ejYcOGWLVqFX777TeULVv2vcf09/dHSEgIZs2ahS+++AIxMTG4fv06gDdf5rVr18bevXtRuXJlGBgYAABCQkIQEBCAefPmwc3NDefOnUP//v1hYmICX19fJCUloU2bNmjcuDFWr16NqKgoDBs2LI8/PSLKdxI//ZCoUHv3McQnT54UFhYWokuXLiIgIEDo6+uLx48fq9bv27dPFC9eXKSkpKjtx9nZWSxevFgIIUS9evXEN998o7a+Tp06onr16tkeNyEhQSiVShESEpJtjFFRUQKAOHfunFq7g4ODWLNmjVrbL7/8IurVqyeEEGLx4sXC3NxcJCUlqdYvXLgw230RUeHF0gDRJ/r7779hamoKQ0ND1KtXD40aNcLcuXMBAE5OTrCyslL1PXv2LBITE2FhYQFTU1PVEhUVhdu3bwMArl27hnr16qkd493X/3Xt2jWkpqaiSZMmOY75yZMnuH//Pvz8/NTimDRpkloc1atXh7GxcY7iIKLCiaUBok/k5eWFhQsXQl9fH/b29moTAk1MTNT6ZmZmws7ODuHh4Vn2U6JEiVwd38jISONtMjMzAbwpD9SpU0dt3dsShhAiV/EQUeHCRIDoE5mYmMDFxSVHfd3d3REbGws9PT2ULl062z4VK1bEiRMn0KtXL1XbiRMn3rvPcuXKwcjICPv27UO/fv2yrH87JyAjI0PVZmNjg88++wx37txBjx49st1vpUqVsGrVKiQnJ6uSjQ/FQUSFE0sDRPmoadOmqFevHjp06IBdu3YhOjoax44dw08//YQzZ84AAIYNG4bQ0FCEhobi5s2bCAgIwJUrV967T0NDQ4wdOxZjxozBypUrcfv2bZw4cQJLly4FAFhbW8PIyAg7d+7Ev//+i/j4eABvblIUFBSEOXPm4ObNm7h06RKWLVuGmTNnAgC6d+8OHR0d+Pn54erVq9ixYwd+/fXXPP6EiCi/MREgykcKhQI7duxAo0aN0LdvX7i6uqJr166Ijo6GjY0NAMDHxwc///wzxo4di5o1a+Lu3bv49ttvP7jf8ePHY9SoUfj5559RsWJF+Pj44PHjxwAAPT09/Pbbb1i8eDHs7e3Rvn17AEC/fv2wZMkSLF++HFWrVoWHhweWL1+uutzQ1NQU27Ztw9WrV+Hm5oYff/wR06ZNy8NPh4ikoBAsBBIREckWRwSIiIhkjIkAERGRjDERICIikjEmAkRERDLGRICIiEjGmAgQERHJGBMBIiIiGWMiQEREJGNMBIiIiGSMiQAREZGMMREgIiKSsf8DSnb6/Uxpe7QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"No\", \"Moderate\", \"Severe\"],\n",
    "            yticklabels=[\"No\", \"Moderate\", \"Severe\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a3ac3d4c-bdea-4bf1-b6b5-c5228e6d0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " No stenosis       0.80      0.62      0.70       305\n",
      "    Moderate       0.15      0.27      0.19        52\n",
      "      Severe       0.11      0.19      0.14        37\n",
      "\n",
      "    accuracy                           0.53       394\n",
      "   macro avg       0.35      0.36      0.34       394\n",
      "weighted avg       0.65      0.53      0.58       394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=[\"No stenosis\", \"Moderate\", \"Severe\"]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d604a28e-235f-4e6d-98f0-8f3c90d0b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# STEP 1 — Ordinal label preparation\n",
    "# ------------------------------------\n",
    "\n",
    "def add_ordinal_targets(df, label_col=\"final_stenosis_label\"):\n",
    "    \"\"\"\n",
    "    Adds ordinal targets:\n",
    "    ord_ge_1: stenosis >= Moderate\n",
    "    ord_ge_2: stenosis >= Severe\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure integer labels\n",
    "    df[label_col] = df[label_col].astype(int)\n",
    "\n",
    "    # Ordinal thresholds\n",
    "    df[\"ord_ge_1\"] = (df[label_col] >= 1).astype(int)\n",
    "    df[\"ord_ge_2\"] = (df[label_col] >= 2).astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "414410a8-6810-40a1-a7a0-8c997763b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_ordinal_targets(train_df)\n",
    "val_df   = add_ordinal_targets(val_df)\n",
    "test_df  = add_ordinal_targets(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "651fa134-9293-490f-95d2-e397c2e97ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ordinal distribution:\n",
      "                      ord_ge_1  ord_ge_2\n",
      "final_stenosis_label                    \n",
      "0                            0         0\n",
      "1                          201         0\n",
      "2                          201       201\n",
      "\n",
      "✅ STEP 2 DONE → Saved: /work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Ordinal_model/FINAL_DATASET_ORDINAL.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# STEP 2 — ORDINAL LABEL CREATION\n",
    "# ==============================\n",
    "\n",
    "INPUT_CSV  = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Files/FINAL_DATASET_MODEL_READY.csv\"\n",
    "OUTPUT_CSV = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Ordinal_model/FINAL_DATASET_ORDINAL.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# ---- sanity check ----\n",
    "assert \"final_stenosis_label\" in df.columns, \"Missing final_stenosis_label column\"\n",
    "\n",
    "# ---- remove NaNs (you decided this earlier) ----\n",
    "df = df.dropna(subset=[\"final_stenosis_label\"]).reset_index(drop=True)\n",
    "\n",
    "# ---- ensure integer ----\n",
    "df[\"final_stenosis_label\"] = df[\"final_stenosis_label\"].astype(int)\n",
    "\n",
    "# ---- ordinal encoding ----\n",
    "df[\"ord_ge_1\"] = (df[\"final_stenosis_label\"] >= 1).astype(int)\n",
    "df[\"ord_ge_2\"] = (df[\"final_stenosis_label\"] >= 2).astype(int)\n",
    "\n",
    "# ---- verify distribution ----\n",
    "print(\"\\nOrdinal distribution:\")\n",
    "print(\n",
    "    df.groupby(\"final_stenosis_label\")[[\"ord_ge_1\", \"ord_ge_2\"]]\n",
    "      .sum()\n",
    ")\n",
    "\n",
    "# ---- save ----\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(\"\\n✅ STEP 2 DONE → Saved:\", OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "34bee85d-b117-4386-bdee-a063a6d53696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'roi_path', 'jet_area', 'jet_width', 'jet_height', 'jet_length', 'red_ratio', 'orange_yellow_ratio', 'blue_ratio', 'PS', 'ED', 'Side', 'prox_ica_right_stenosis', 'prox_ica_left_stenosis', 'detected_side_value', 'Patient_Number', 'final_stenosis_label', 'side_enc', 'PS_missing', 'ED_missing', 'ord_ge_1', 'ord_ge_2']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "fa1ed9cf-8809-49cf-b9e7-33d70aa6be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded\n",
      "Total samples: 1856\n",
      "Unique patients: 50\n",
      "\n",
      "Label distribution:\n",
      "final_stenosis_label\n",
      "0    1454\n",
      "1     201\n",
      "2     201\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ SPLIT COMPLETE\n",
      "Train: 1295 samples | 34 patients\n",
      "Val  : 296 samples | 8 patients\n",
      "Test : 265 samples | 8 patients\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# =========================\n",
    "# FILE PATHS\n",
    "# =========================\n",
    "INPUT_CSV = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Ordinal_model/FINAL_DATASET_ORDINAL.csv\"\n",
    "\n",
    "TRAIN_CSV = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Ordinal_model/train.csv\"\n",
    "VAL_CSV   = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Ordinal_model/val.csv\"\n",
    "TEST_CSV  = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Ordinal_model/test.csv\"\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA\n",
    "# =========================\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# =========================\n",
    "# SANITY CHECKS\n",
    "# =========================\n",
    "required_cols = [\n",
    "    \"roi_path\",\n",
    "    \"Patient_Number\",\n",
    "    \"final_stenosis_label\",\n",
    "    \"ord_ge_1\",\n",
    "    \"ord_ge_2\"\n",
    "]\n",
    "\n",
    "for col in required_cols:\n",
    "    assert col in df.columns, f\"Missing column: {col}\"\n",
    "\n",
    "print(\"✅ Dataset loaded\")\n",
    "print(\"Total samples:\", len(df))\n",
    "print(\"Unique patients:\", df[\"Patient_Number\"].nunique())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df[\"final_stenosis_label\"].value_counts())\n",
    "\n",
    "# =========================\n",
    "# SPLIT 1 — TEST (15%)\n",
    "# =========================\n",
    "gss_test = GroupShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "trainval_idx, test_idx = next(\n",
    "    gss_test.split(df, groups=df[\"Patient_Number\"])\n",
    ")\n",
    "\n",
    "trainval_df = df.iloc[trainval_idx].reset_index(drop=True)\n",
    "test_df     = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# SPLIT 2 — TRAIN / VAL\n",
    "# (15% val from remaining → ~70/15/15)\n",
    "# =========================\n",
    "gss_val = GroupShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=0.176,  # 0.176 × 85% ≈ 15%\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_idx, val_idx = next(\n",
    "    gss_val.split(trainval_df, groups=trainval_df[\"Patient_Number\"])\n",
    ")\n",
    "\n",
    "train_df = trainval_df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = trainval_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# SAVE SPLITS\n",
    "# =========================\n",
    "train_df.to_csv(TRAIN_CSV, index=False)\n",
    "val_df.to_csv(VAL_CSV, index=False)\n",
    "test_df.to_csv(TEST_CSV, index=False)\n",
    "\n",
    "# =========================\n",
    "# REPORT\n",
    "# =========================\n",
    "print(\"\\n✅ SPLIT COMPLETE\")\n",
    "print(f\"Train: {len(train_df)} samples | {train_df['Patient_Number'].nunique()} patients\")\n",
    "print(f\"Val  : {len(val_df)} samples | {val_df['Patient_Number'].nunique()} patients\")\n",
    "print(f\"Test : {len(test_df)} samples | {test_df['Patient_Number'].nunique()} patients\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fef458b-caa3-4e40-bcf0-a241c6c35564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1892d032-5f48-4352-9f39-c2e23fa5db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Ordinal_model/ordinal_train.csv\"\n",
    "VAL_CSV   = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Ordinal_model/ordinal_val.csv\"\n",
    "TEST_CSV = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Ordinal_model/ordinal_test.csv\"\n",
    "ROOT_IMG = \"/work/SanzidaAkterChadne#0992/Jobs/JupyterLab/5486489/main/Raw_data_stenosis/stenosis_project/data/Extracted_roi_folder\"  # not used directly, roi_path is used\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "100a5c2f-da86-4737-9290-ab74d016954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB_FEATURES = [\n",
    "    \"jet_area\", \"jet_width\", \"jet_height\", \"jet_length\",\n",
    "    \"red_ratio\", \"orange_yellow_ratio\", \"blue_ratio\",\n",
    "    \"side_enc\", \"PS_missing\", \"ED_missing\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54103ac9-ce67-4ee3-8703-d1a0b6feda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c2e021e-1d21-4286-ba16-b93dbb82e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalMultimodalDataset(Dataset):\n",
    "    def __init__(self, df, root_img, tab_features, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root_img = root_img\n",
    "        self.tab_features = tab_features\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # IMAGE\n",
    "        img_path = row[\"roi_path\"]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # TABULAR (FORCE FLOAT)\n",
    "        tab = torch.tensor(\n",
    "            row[self.tab_features].astype(float).values,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # ORDINAL TARGET (ALWAYS SIZE 2)\n",
    "        target = torch.tensor(\n",
    "            [row[\"ord_ge_1\"], row[\"ord_ge_2\"]],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        return image, tab, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a860822f-bf08-475e-bd94-bc51ad61e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OrdinalMultimodalDataset(\n",
    "    train_df, ROOT_IMG, TAB_FEATURES, image_transforms\n",
    ")\n",
    "val_dataset = OrdinalMultimodalDataset(\n",
    "    val_df, ROOT_IMG, TAB_FEATURES, image_transforms\n",
    ")\n",
    "test_dataset = OrdinalMultimodalDataset(\n",
    "    test_df, ROOT_IMG, TAB_FEATURES, image_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12920523-4316-4bb2-ae0d-40d64ea6b451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([16, 3, 224, 224])\n",
      "Tabular: torch.Size([16, 10])\n",
      "Targets: torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "images, tabs, targets = next(iter(train_loader))\n",
    "print(\"Images:\", images.shape)\n",
    "print(\"Tabular:\", tabs.shape)\n",
    "print(\"Targets:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d592bf36-f519-4ef3-a260-1116077c91c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet_area                 int64\n",
      "jet_width                int64\n",
      "jet_height               int64\n",
      "jet_length             float64\n",
      "red_ratio              float64\n",
      "orange_yellow_ratio    float64\n",
      "blue_ratio             float64\n",
      "side_enc                 int64\n",
      "PS_missing               int64\n",
      "ED_missing               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df[TAB_FEATURES].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8769e4e3-cc18-4e1b-86f5-ced09e1dc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalMultiModalNet(nn.Module):\n",
    "    def __init__(self, tab_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")\n",
    "        self.cnn.classifier = nn.Identity()\n",
    "        cnn_out = 1280\n",
    "\n",
    "        self.tabular = nn.Sequential(\n",
    "            nn.Linear(tab_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(cnn_out + 64, 2)\n",
    "\n",
    "    def forward(self, image, tab):\n",
    "        img_feat = self.cnn(image)\n",
    "        tab_feat = self.tabular(tab)\n",
    "        fused = torch.cat([img_feat, tab_feat], dim=1)\n",
    "        return self.classifier(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78a1fefb-ae9d-4c2d-8ec6-91bfc84716ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_logits_to_class(logits):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    return (probs > 0.5).sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae844e0c-ceb2-4cd5-b7da-8135bf69b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = OrdinalMultiModalNet(tab_dim=len(TAB_FEATURES)).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b79a37b1-e00c-409f-8182-919634b4e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    progress = tqdm(loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for images, tabs, targets in progress:\n",
    "        images = images.to(device)\n",
    "        tabs = tabs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images, tabs)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = ordinal_logits_to_class(logits)\n",
    "        true  = ordinal_logits_to_class(targets)\n",
    "\n",
    "        correct += (preds == true).sum().item()\n",
    "        total += images.size(0)\n",
    "\n",
    "        progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    progress = tqdm(loader, desc=\"Validating\", leave=False)\n",
    "\n",
    "    for images, tabs, targets in progress:\n",
    "        images = images.to(device)\n",
    "        tabs = tabs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits = model(images, tabs)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = ordinal_logits_to_class(logits)\n",
    "        true  = ordinal_logits_to_class(targets)\n",
    "\n",
    "        correct += (preds == true).sum().item()\n",
    "        total += images.size(0)\n",
    "\n",
    "        progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50d202c4-8d27-47ab-876b-c582f32c6db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss 0.1830 Acc 0.892 | Val Loss 0.3695 Acc 0.821\n",
      "✅ Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss 0.2190 Acc 0.868 | Val Loss 0.4586 Acc 0.811\n",
      "⚠ No improvement 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Loss 0.2843 Acc 0.839 | Val Loss 0.3847 Acc 0.818\n",
      "⚠ No improvement 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Loss 0.2347 Acc 0.876 | Val Loss 0.5887 Acc 0.716\n",
      "⚠ No improvement 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Loss 0.1410 Acc 0.913 | Val Loss 0.4072 Acc 0.801\n",
      "⚠ No improvement 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Loss 0.2297 Acc 0.901 | Val Loss 0.4949 Acc 0.767\n",
      "⚠ No improvement 5/5\n",
      "🛑 Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "best_val_loss = float(\"inf\")\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Loss {train_loss:.4f} Acc {train_acc:.3f} | \"\n",
    "        f\"Val Loss {val_loss:.4f} Acc {val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_ordinal_multimodal.pth\")\n",
    "        no_improve = 0\n",
    "        print(\"✅ Saved best model\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print(f\"⚠ No improvement {no_improve}/{PATIENCE}\")\n",
    "\n",
    "    if no_improve >= PATIENCE:\n",
    "        print(\"🛑 Early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17e0a467-de3b-4ef8-9fa6-b53f0001243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model loaded for TEST evaluation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(torch.load(\"best_ordinal_multimodal.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Best model loaded for TEST evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b43b0b8-2ea5-46c5-b8d8-6bbbdb41abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def ordinal_logits_to_class(logits, threshold=0.5):\n",
    "    \"\"\"\n",
    "    logits: Tensor [B, 2]\n",
    "    returns: Tensor [B] with class {0,1,2}\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > threshold).int()\n",
    "    # number of passed thresholds = class\n",
    "    return preds.sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e1ab801-3444-4c08-a0a6-a58cd927aa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Test Accuracy: 0.6792\n",
      "⚖ Balanced Accuracy: 0.3565\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(\n",
    "    torch.load(\"best_ordinal_multimodal.pth\", map_location=device)\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, tabs, targets in test_loader:\n",
    "        images = images.to(device)\n",
    "        tabs = tabs.to(device)\n",
    "        targets = targets.to(device)          # shape [B, 2]\n",
    "\n",
    "        logits = model(images, tabs)           # shape [B, 2]\n",
    "\n",
    "        # ordinal → class\n",
    "        preds = ordinal_logits_to_class(logits)\n",
    "        true  = targets.sum(dim=1).long()\n",
    "\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_targets.append(true.cpu())\n",
    "\n",
    "y_pred = torch.cat(all_preds).numpy()\n",
    "y_true = torch.cat(all_targets).numpy()\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"🎯 Test Accuracy: {acc:.4f}\")\n",
    "print(f\"⚖ Balanced Accuracy: {bal_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f190cfbe-1735-44a3-b20b-5b5d29192c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[178  15   8]\n",
      " [ 35   1  22]\n",
      " [  4   1   1]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No stenosis      0.820     0.886     0.852       201\n",
      "    Moderate      0.059     0.017     0.027        58\n",
      "      Severe      0.032     0.167     0.054         6\n",
      "\n",
      "    accuracy                          0.679       265\n",
      "   macro avg      0.304     0.356     0.311       265\n",
      "weighted avg      0.636     0.679     0.653       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=[\"No stenosis\", \"Moderate\", \"Severe\"],\n",
    "        digits=3\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d494c60-cea2-402a-a1e9-486407d20b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"No\", \"Moderate\", \"Severe\"],\n",
    "            yticklabels=[\"No\", \"Moderate\", \"Severe\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ea577-7ce7-441b-aa86-def643647feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
